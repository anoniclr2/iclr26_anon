{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6dfa600",
   "metadata": {},
   "source": [
    "# AURAC Tables for April 2025 Data\n",
    "\n",
    "This notebook computes AURAC for the April 2025 experimental data and generates publication-ready LaTeX tables.\n",
    "\n",
    "**Prerequisites:** \n",
    "- April 2025 CSV files with experimental results\n",
    "- AURAC computation (this notebook will compute it)\n",
    "\n",
    "**Output:** \n",
    "- Two properly formatted LaTeX tables (Full and Mini datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c393ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ttest_ind_from_stats, norm\n",
    "\n",
    "sys.path.append('../src/')\n",
    "from plotting import get_coverage_threshold_and_size, get_auc, compute_aurac\n",
    "\n",
    "# Base path setup\n",
    "base_path = Path('Ensembling_Finetuned_LLMs')\n",
    "\n",
    "def read_file(file_path, base_path=base_path) -> pd.DataFrame:\n",
    "    \"\"\"Read a file and return a DataFrame.\"\"\"\n",
    "    path = base_path / 'llm_experiments_data' / file_path\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File {path} does not exist.\")\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def combine_and_clean_dataframes(df1, df2=None) -> pd.DataFrame:\n",
    "    \"\"\"Combine and clean the dataframes.\"\"\"\n",
    "    parts = [df1]\n",
    "    if df2 is not None:\n",
    "        parts.append(df2)\n",
    "    combined_df = pd.concat(parts, ignore_index=True)\n",
    "    combined_df = combined_df.drop_duplicates(subset=['dataset', 'seed', 'method', 'ensemble_type'], keep='first')\n",
    "    combined_df.reset_index(drop=True, inplace=True)\n",
    "    return combined_df\n",
    "\n",
    "def calc_ci_for_df(df) -> pd.DataFrame:\n",
    "    \"\"\"Calculate confidence intervals for DataFrame (AURAC-focused version).\"\"\"\n",
    "    # Only use columns that exist in AURAC-enriched data (removed threshold, set_size, auc)\n",
    "    available_columns = ['ensemble_size', 'ensemble_unique_size', 'nll_test', 'c1', 'c2', 'epi_scalar', 'aurac', 'aorac']\n",
    "    \n",
    "    df = df.groupby(['dataset', 'method', 'ensemble_type'])[available_columns].agg(['mean', 'std', 'count']).reset_index()\n",
    "    \n",
    "    for col in available_columns:\n",
    "        df[(col, 'CI')] = 1.96 * (df[(col, 'std')] / np.sqrt(df[(col, 'count')].replace(0, np.nan)))\n",
    "        df[(col, 'mean±CI')] = (df[(col, 'mean')].round(4).astype(str) + \n",
    "                              \" ± \" + df[(col, 'CI')].round(4).astype(str))\n",
    "    return df\n",
    "\n",
    "def flatten_subset_df(df, subset=['dataset', 'method', 'ensemble_type', 'aurac_mean±CI']) -> pd.DataFrame:\n",
    "    \"\"\"Flatten the dataframe (AURAC-focused version).\"\"\"\n",
    "    df.columns = ['_'.join(col).strip('_') if col[1] else col[0] for col in df.columns.values]\n",
    "    return df[subset]\n",
    "\n",
    "def compare_to_best(df: pd.DataFrame, metric: str, ensemble_types: list = None, \n",
    "                   calibration_method: str = 'pure_logits', alpha: float = 0.05) -> pd.DataFrame:\n",
    "    \"\"\"Compare all ensemble types to the best one for each dataset.\"\"\"\n",
    "    z = norm.ppf(1 - alpha/2)\n",
    "    \n",
    "    if ensemble_types is None:\n",
    "        ensemble_types = df['ensemble_type'].unique().tolist()\n",
    "    \n",
    "    results = []\n",
    "    for ds in df['dataset'].unique():\n",
    "        sub = df[(df['dataset'] == ds) & (df['method'] == calibration_method) & \n",
    "                (df['ensemble_type'].isin(ensemble_types))].copy()\n",
    "        \n",
    "        if sub.empty:\n",
    "            continue\n",
    "            \n",
    "        sub[f'{metric}_ci_lo'] = sub[f'{metric}_mean'] - z * sub[f'{metric}_std'] / np.sqrt(sub[f'{metric}_count'])\n",
    "        sub[f'{metric}_ci_hi'] = sub[f'{metric}_mean'] + z * sub[f'{metric}_std'] / np.sqrt(sub[f'{metric}_count'])\n",
    "        \n",
    "        if metric in ['auc', 'aurac']:\n",
    "            best_idx = sub[f'{metric}_mean'].idxmax()\n",
    "        else:\n",
    "            best_idx = sub[f'{metric}_mean'].idxmin()\n",
    "        best = sub.loc[best_idx]\n",
    "        \n",
    "        for _, row in sub.iterrows():\n",
    "            m1, s1, n1 = row[f'{metric}_mean'], row[f'{metric}_std'], row[f'{metric}_count']\n",
    "            m0, s0, n0 = best[f'{metric}_mean'], best[f'{metric}_std'], best[f'{metric}_count']\n",
    "            \n",
    "            t_stat, p_val = ttest_ind_from_stats(mean1=m1, std1=s1, nobs1=n1,\n",
    "                                               mean2=m0, std2=s0, nobs2=n0, equal_var=False)\n",
    "            \n",
    "            results.append({\n",
    "                'dataset': ds, 'ensemble_type': row['ensemble_type'],\n",
    "                'best_ensemble_type': best['ensemble_type'], 'mean': m1,\n",
    "                'mean_best': m0, 'significant': (p_val < alpha)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def create_report_view_df(df: pd.DataFrame, value_vars: list, \n",
    "                         calibration_method: str = 'pure_logits', custom_rows: list = None) -> pd.DataFrame:\n",
    "    \"\"\"Create wide-format DataFrame for reporting.\"\"\"\n",
    "    df = df.loc[df['method'] == calibration_method]\n",
    "    \n",
    "    dfm = df.melt(id_vars=['dataset', 'ensemble_type', 'method'], \n",
    "                 value_vars=value_vars, var_name='metric', value_name='value')\n",
    "    \n",
    "    metric_name_map = {'aurac_mean±CI': 'AURAC'}\n",
    "    dfm['metric'] = dfm['metric'].map(metric_name_map)\n",
    "    dfm['dataset'] = dfm['dataset'].map({\n",
    "        'SetFit/mnli': 'SetFit', 'ag_news': 'News', 'dbpedia_14': 'DBpedia',\n",
    "        'imdb': 'IMDB', 'mteb/tweet_sentiment_extraction': 'Tweet', 'stanfordnlp/sst2': 'SST-2'\n",
    "    })\n",
    "    \n",
    "    pivoted = dfm.pivot_table(index=['ensemble_type', 'method'], columns=['metric', 'dataset'],\n",
    "                             values='value', aggfunc='first')\n",
    "    \n",
    "    metrics_order = [metric_name_map[v] for v in value_vars]\n",
    "    datasets_order = ['DBpedia', 'News', 'SST-2', 'SetFit', 'Tweet', 'IMDB']\n",
    "    new_cols = pd.MultiIndex.from_product([metrics_order, datasets_order], names=['metric', 'dataset'])\n",
    "    \n",
    "    wide = pivoted.reindex(columns=new_cols).reset_index()\n",
    "    \n",
    "    if custom_rows is None:\n",
    "        custom_rows = sorted(df['ensemble_type'].unique())\n",
    "    \n",
    "    wide['ensemble_type'] = pd.Categorical(wide['ensemble_type'], categories=custom_rows, ordered=True)\n",
    "    wide = wide.sort_values(['ensemble_type', 'method']).reset_index(drop=True)\n",
    "    \n",
    "    return wide\n",
    "\n",
    "print(\"Functions loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65050b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding AURAC to existing April 2025 data...\n",
      "Mini 1 shape: (312, 11)\n",
      "Mini 2 shape: (160, 11)\n",
      "FTC shape: (480, 11)\n",
      "AURAC files already exist. Skipping computation.\n",
      "Output files:\n",
      "  - Ensembling_Finetuned_LLMs/llm_experiments_data/metrics/mini_1_with_aurac_cov_0p99_09_24.csv\n",
      "  - Ensembling_Finetuned_LLMs/llm_experiments_data/metrics/mini_2_with_aurac_cov_0p99_09_24.csv\n",
      "  - Ensembling_Finetuned_LLMs/llm_experiments_data/metrics/ftc_with_aurac_cov_0p99_09_24.csv\n"
     ]
    }
   ],
   "source": [
    "# Compute ONLY AURAC for April 2025 data (other metrics already exist)\n",
    "\n",
    "def compute_aurac_only(row, base_path=base_path) -> float:\n",
    "    \"\"\"Compute only AURAC for a single row.\"\"\"\n",
    "    npfile = base_path / row['path']\n",
    "    data = np.load(npfile, allow_pickle=True)\n",
    "    probs, labels = data['ensemble_probs'], data['labels']\n",
    "    aurac = compute_aurac(labels, probs)\n",
    "    return aurac\n",
    "\n",
    "def add_aurac_to_existing_data(df, base_path=base_path, output_path=None) -> None:\n",
    "    \"\"\"Add AURAC column to existing DataFrame with other metrics.\"\"\"\n",
    "    df = df.copy()\n",
    "    print(f\"Computing AURAC for {len(df)} rows...\")\n",
    "    tqdm.pandas(desc=\"Computing AURAC only\")\n",
    "    df['aurac'] = df.progress_apply(compute_aurac_only, axis=1, base_path=base_path)\n",
    "    df['aorac'] = 1 - df['aurac']  # Also compute AORAC since it's trivial\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"AURAC added and saved to {output_path}\")\n",
    "\n",
    "# Load original data files (these should already have other metrics)\n",
    "mini_path_1_orig = 'llm_experimental_results_mini_neurips_2025-04-20.csv'\n",
    "mini_path_2_orig = 'llm_experimental_results_mini_neurips_2025-04-24.csv'\n",
    "ftc_path_orig = 'llm_experimental_results_ftc_neurips_2025-04-25.csv'\n",
    "\n",
    "print(\"Adding AURAC to existing April 2025 data...\")\n",
    "\n",
    "# Generate output paths consistently (outside try/except block)\n",
    "from datetime import datetime\n",
    "date_suffix = datetime.now().strftime(\"%m_%d\")\n",
    "\n",
    "output_path_mini_1_aurac_99 = base_path / 'llm_experiments_data' / 'metrics' / f'mini_1_with_aurac_cov_0p99_{date_suffix}.csv'\n",
    "output_path_mini_2_aurac_99 = base_path / 'llm_experiments_data' / 'metrics' / f'mini_2_with_aurac_cov_0p99_{date_suffix}.csv'\n",
    "output_path_ftc_aurac_99 = base_path / 'llm_experiments_data' / 'metrics' / f'ftc_with_aurac_cov_0p99_{date_suffix}.csv'\n",
    "\n",
    "try:\n",
    "    df_mini_1_orig = read_file(mini_path_1_orig)\n",
    "    df_mini_2_orig = read_file(mini_path_2_orig)\n",
    "    df_ftc_orig = read_file(ftc_path_orig)\n",
    "\n",
    "    print(f\"Mini 1 shape: {df_mini_1_orig.shape}\")\n",
    "    print(f\"Mini 2 shape: {df_mini_2_orig.shape}\")\n",
    "    print(f\"FTC shape: {df_ftc_orig.shape}\")\n",
    "    \n",
    "    # Check if AURAC files already exist\n",
    "    if output_path_ftc_aurac_99.exists():\n",
    "        print(\"AURAC files already exist. Skipping computation.\")\n",
    "    else:\n",
    "        # Only compute AURAC (much faster!)\n",
    "        add_aurac_to_existing_data(df_mini_1_orig, base_path=base_path, output_path=output_path_mini_1_aurac_99)\n",
    "        add_aurac_to_existing_data(df_mini_2_orig, base_path=base_path, output_path=output_path_mini_2_aurac_99)\n",
    "        add_aurac_to_existing_data(df_ftc_orig, base_path=base_path, output_path=output_path_ftc_aurac_99)\n",
    "\n",
    "        print(\"AURAC computation completed (only AURAC computed, other metrics preserved)\")\n",
    "        \n",
    "    print(\"Output files:\")\n",
    "    print(f\"  - {output_path_mini_1_aurac_99}\")\n",
    "    print(f\"  - {output_path_mini_2_aurac_99}\")\n",
    "    print(f\"  - {output_path_ftc_aurac_99}\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Original April 2025 files not found: {e}\")\n",
    "    print(\"This is normal if you only need tables from the newer data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea04f35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FTC data loaded: (480, 13)\n",
      "Unique ensemble types: ['greedy_50_baseline', 'greedy_50_calib_every_step', 'greedy_50_calib_once', 'greedy_50_post_calib', 'greedy_50_temp_baseline', 'greedy_unique_5_baseline', 'greedy_unique_5_post_calib', 'greedy_unique_5_temp_baseline']\n",
      "Processed FTC data: (96, 4)\n",
      "Mini 1 data loaded: (312, 13)\n",
      "Mini 2 data loaded: (160, 13)\n",
      "Combined Mini data: (472, 13)\n",
      "Processed Mini data: (96, 4)\n",
      "Running significance tests for AURAC metric...\n",
      "FTC AURAC significance results: (48, 6)\n",
      "Mini AURAC significance results: (48, 6)\n",
      "Data processing completed\n"
     ]
    }
   ],
   "source": [
    "# Process FTC data and compute significance\n",
    "df_ftc_aurac = pd.read_csv(output_path_ftc_aurac_99)\n",
    "print(f\"FTC data loaded: {df_ftc_aurac.shape}\")\n",
    "print(f\"Unique ensemble types: {sorted(df_ftc_aurac['ensemble_type'].unique())}\")\n",
    "\n",
    "# Clean and calculate CIs\n",
    "df_ftc_aurac_clean = combine_and_clean_dataframes(df_ftc_aurac)\n",
    "df_ftc_aurac_ci = calc_ci_for_df(df_ftc_aurac_clean)\n",
    "df_ftc_aurac_subset = flatten_subset_df(df_ftc_aurac_ci, subset=['dataset', 'method', 'ensemble_type', 'aurac_mean±CI'])\n",
    "\n",
    "print(f\"Processed FTC data: {df_ftc_aurac_subset.shape}\")\n",
    "\n",
    "# Process Mini data (combine mini_1 and mini_2)\n",
    "df_mini_1_aurac = pd.read_csv(output_path_mini_1_aurac_99)\n",
    "df_mini_2_aurac = pd.read_csv(output_path_mini_2_aurac_99)\n",
    "\n",
    "print(f\"Mini 1 data loaded: {df_mini_1_aurac.shape}\")\n",
    "print(f\"Mini 2 data loaded: {df_mini_2_aurac.shape}\")\n",
    "\n",
    "# Combine mini datasets\n",
    "df_mini_aurac_combined = combine_and_clean_dataframes(df_mini_1_aurac, df_mini_2_aurac)\n",
    "print(f\"Combined Mini data: {df_mini_aurac_combined.shape}\")\n",
    "\n",
    "# Calculate CIs\n",
    "df_mini_aurac_ci = calc_ci_for_df(df_mini_aurac_combined)\n",
    "df_mini_aurac_subset = flatten_subset_df(df_mini_aurac_ci, subset=['dataset', 'method', 'ensemble_type', 'aurac_mean±CI'])\n",
    "\n",
    "print(f\"Processed Mini data: {df_mini_aurac_subset.shape}\")\n",
    "\n",
    "# Run significance tests\n",
    "print(\"Running significance tests for AURAC metric...\")\n",
    "\n",
    "df_ftc_significance_aurac = compare_to_best(df_ftc_aurac_ci, metric='aurac', calibration_method='pure_logits', alpha=0.05)\n",
    "df_mini_significance_aurac = compare_to_best(df_mini_aurac_ci, metric='aurac', calibration_method='pure_logits', alpha=0.05)\n",
    "\n",
    "print(f\"FTC AURAC significance results: {df_ftc_significance_aurac.shape}\")\n",
    "print(f\"Mini AURAC significance results: {df_mini_significance_aurac.shape}\")\n",
    "\n",
    "print(\"Data processing completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1d47743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FTC AURAC Significance Tests ===\n",
      "FTC AURAC significance results:\n",
      "                            dataset                  ensemble_type  \\\n",
      "0                       SetFit/mnli             greedy_50_baseline   \n",
      "1                       SetFit/mnli     greedy_50_calib_every_step   \n",
      "2                       SetFit/mnli           greedy_50_calib_once   \n",
      "3                       SetFit/mnli           greedy_50_post_calib   \n",
      "4                       SetFit/mnli        greedy_50_temp_baseline   \n",
      "5                       SetFit/mnli       greedy_unique_5_baseline   \n",
      "6                       SetFit/mnli     greedy_unique_5_post_calib   \n",
      "7                       SetFit/mnli  greedy_unique_5_temp_baseline   \n",
      "8                           ag_news             greedy_50_baseline   \n",
      "9                           ag_news     greedy_50_calib_every_step   \n",
      "10                          ag_news           greedy_50_calib_once   \n",
      "11                          ag_news           greedy_50_post_calib   \n",
      "12                          ag_news        greedy_50_temp_baseline   \n",
      "13                          ag_news       greedy_unique_5_baseline   \n",
      "14                          ag_news     greedy_unique_5_post_calib   \n",
      "15                          ag_news  greedy_unique_5_temp_baseline   \n",
      "16                       dbpedia_14             greedy_50_baseline   \n",
      "17                       dbpedia_14     greedy_50_calib_every_step   \n",
      "18                       dbpedia_14           greedy_50_calib_once   \n",
      "19                       dbpedia_14           greedy_50_post_calib   \n",
      "20                       dbpedia_14        greedy_50_temp_baseline   \n",
      "21                       dbpedia_14       greedy_unique_5_baseline   \n",
      "22                       dbpedia_14     greedy_unique_5_post_calib   \n",
      "23                       dbpedia_14  greedy_unique_5_temp_baseline   \n",
      "24                             imdb             greedy_50_baseline   \n",
      "25                             imdb     greedy_50_calib_every_step   \n",
      "26                             imdb           greedy_50_calib_once   \n",
      "27                             imdb           greedy_50_post_calib   \n",
      "28                             imdb        greedy_50_temp_baseline   \n",
      "29                             imdb       greedy_unique_5_baseline   \n",
      "30                             imdb     greedy_unique_5_post_calib   \n",
      "31                             imdb  greedy_unique_5_temp_baseline   \n",
      "32  mteb/tweet_sentiment_extraction             greedy_50_baseline   \n",
      "33  mteb/tweet_sentiment_extraction     greedy_50_calib_every_step   \n",
      "34  mteb/tweet_sentiment_extraction           greedy_50_calib_once   \n",
      "35  mteb/tweet_sentiment_extraction           greedy_50_post_calib   \n",
      "36  mteb/tweet_sentiment_extraction        greedy_50_temp_baseline   \n",
      "37  mteb/tweet_sentiment_extraction       greedy_unique_5_baseline   \n",
      "38  mteb/tweet_sentiment_extraction     greedy_unique_5_post_calib   \n",
      "39  mteb/tweet_sentiment_extraction  greedy_unique_5_temp_baseline   \n",
      "40                 stanfordnlp/sst2             greedy_50_baseline   \n",
      "41                 stanfordnlp/sst2     greedy_50_calib_every_step   \n",
      "42                 stanfordnlp/sst2           greedy_50_calib_once   \n",
      "43                 stanfordnlp/sst2           greedy_50_post_calib   \n",
      "44                 stanfordnlp/sst2        greedy_50_temp_baseline   \n",
      "45                 stanfordnlp/sst2       greedy_unique_5_baseline   \n",
      "46                 stanfordnlp/sst2     greedy_unique_5_post_calib   \n",
      "47                 stanfordnlp/sst2  greedy_unique_5_temp_baseline   \n",
      "\n",
      "            best_ensemble_type      mean  mean_best  significant  \n",
      "0         greedy_50_calib_once  0.902264   0.925227         True  \n",
      "1         greedy_50_calib_once  0.922588   0.925227         True  \n",
      "2         greedy_50_calib_once  0.925227   0.925227        False  \n",
      "3         greedy_50_calib_once  0.923693   0.925227         True  \n",
      "4         greedy_50_calib_once  0.902266   0.925227         True  \n",
      "5         greedy_50_calib_once  0.891450   0.925227         True  \n",
      "6         greedy_50_calib_once  0.923969   0.925227         True  \n",
      "7         greedy_50_calib_once  0.891505   0.925227         True  \n",
      "8         greedy_50_calib_once  0.981031   0.983724         True  \n",
      "9         greedy_50_calib_once  0.983710   0.983724        False  \n",
      "10        greedy_50_calib_once  0.983724   0.983724        False  \n",
      "11        greedy_50_calib_once  0.983507   0.983724        False  \n",
      "12        greedy_50_calib_once  0.980993   0.983724         True  \n",
      "13        greedy_50_calib_once  0.980987   0.983724         True  \n",
      "14        greedy_50_calib_once  0.982942   0.983724        False  \n",
      "15        greedy_50_calib_once  0.980964   0.983724         True  \n",
      "16        greedy_50_post_calib  0.989487   0.989687         True  \n",
      "17        greedy_50_post_calib  0.989683   0.989687        False  \n",
      "18        greedy_50_post_calib  0.989674   0.989687        False  \n",
      "19        greedy_50_post_calib  0.989687   0.989687        False  \n",
      "20        greedy_50_post_calib  0.989492   0.989687         True  \n",
      "21        greedy_50_post_calib  0.989528   0.989687         True  \n",
      "22        greedy_50_post_calib  0.989683   0.989687        False  \n",
      "23        greedy_50_post_calib  0.989533   0.989687         True  \n",
      "24    greedy_unique_5_baseline  0.983823   0.985910         True  \n",
      "25    greedy_unique_5_baseline  0.985873   0.985910        False  \n",
      "26    greedy_unique_5_baseline  0.985889   0.985910        False  \n",
      "27    greedy_unique_5_baseline  0.985538   0.985910        False  \n",
      "28    greedy_unique_5_baseline  0.983822   0.985910         True  \n",
      "29    greedy_unique_5_baseline  0.985910   0.985910        False  \n",
      "30    greedy_unique_5_baseline  0.985845   0.985910        False  \n",
      "31    greedy_unique_5_baseline  0.985910   0.985910        False  \n",
      "32        greedy_50_calib_once  0.915749   0.924900         True  \n",
      "33        greedy_50_calib_once  0.924368   0.924900        False  \n",
      "34        greedy_50_calib_once  0.924900   0.924900        False  \n",
      "35        greedy_50_calib_once  0.923618   0.924900        False  \n",
      "36        greedy_50_calib_once  0.915817   0.924900         True  \n",
      "37        greedy_50_calib_once  0.910300   0.924900         True  \n",
      "38        greedy_50_calib_once  0.921094   0.924900         True  \n",
      "39        greedy_50_calib_once  0.910293   0.924900         True  \n",
      "40  greedy_50_calib_every_step  0.983324   0.985046         True  \n",
      "41  greedy_50_calib_every_step  0.985046   0.985046        False  \n",
      "42  greedy_50_calib_every_step  0.984987   0.985046        False  \n",
      "43  greedy_50_calib_every_step  0.984861   0.985046        False  \n",
      "44  greedy_50_calib_every_step  0.983324   0.985046         True  \n",
      "45  greedy_50_calib_every_step  0.984003   0.985046         True  \n",
      "46  greedy_50_calib_every_step  0.984218   0.985046         True  \n",
      "47  greedy_50_calib_every_step  0.984003   0.985046         True  \n",
      "\n",
      "=== Mini AURAC Significance Tests ===\n",
      "Mini AURAC significance results:\n",
      "                            dataset                  ensemble_type  \\\n",
      "0                       SetFit/mnli             greedy_50_baseline   \n",
      "1                       SetFit/mnli     greedy_50_calib_every_step   \n",
      "2                       SetFit/mnli           greedy_50_calib_once   \n",
      "3                       SetFit/mnli           greedy_50_post_calib   \n",
      "4                       SetFit/mnli        greedy_50_temp_baseline   \n",
      "5                       SetFit/mnli       greedy_unique_5_baseline   \n",
      "6                       SetFit/mnli     greedy_unique_5_post_calib   \n",
      "7                       SetFit/mnli  greedy_unique_5_temp_baseline   \n",
      "8                           ag_news             greedy_50_baseline   \n",
      "9                           ag_news     greedy_50_calib_every_step   \n",
      "10                          ag_news           greedy_50_calib_once   \n",
      "11                          ag_news           greedy_50_post_calib   \n",
      "12                          ag_news        greedy_50_temp_baseline   \n",
      "13                          ag_news       greedy_unique_5_baseline   \n",
      "14                          ag_news     greedy_unique_5_post_calib   \n",
      "15                          ag_news  greedy_unique_5_temp_baseline   \n",
      "16                       dbpedia_14             greedy_50_baseline   \n",
      "17                       dbpedia_14     greedy_50_calib_every_step   \n",
      "18                       dbpedia_14           greedy_50_calib_once   \n",
      "19                       dbpedia_14           greedy_50_post_calib   \n",
      "20                       dbpedia_14        greedy_50_temp_baseline   \n",
      "21                       dbpedia_14       greedy_unique_5_baseline   \n",
      "22                       dbpedia_14     greedy_unique_5_post_calib   \n",
      "23                       dbpedia_14  greedy_unique_5_temp_baseline   \n",
      "24                             imdb             greedy_50_baseline   \n",
      "25                             imdb     greedy_50_calib_every_step   \n",
      "26                             imdb           greedy_50_calib_once   \n",
      "27                             imdb           greedy_50_post_calib   \n",
      "28                             imdb        greedy_50_temp_baseline   \n",
      "29                             imdb       greedy_unique_5_baseline   \n",
      "30                             imdb     greedy_unique_5_post_calib   \n",
      "31                             imdb  greedy_unique_5_temp_baseline   \n",
      "32  mteb/tweet_sentiment_extraction             greedy_50_baseline   \n",
      "33  mteb/tweet_sentiment_extraction     greedy_50_calib_every_step   \n",
      "34  mteb/tweet_sentiment_extraction           greedy_50_calib_once   \n",
      "35  mteb/tweet_sentiment_extraction           greedy_50_post_calib   \n",
      "36  mteb/tweet_sentiment_extraction        greedy_50_temp_baseline   \n",
      "37  mteb/tweet_sentiment_extraction       greedy_unique_5_baseline   \n",
      "38  mteb/tweet_sentiment_extraction     greedy_unique_5_post_calib   \n",
      "39  mteb/tweet_sentiment_extraction  greedy_unique_5_temp_baseline   \n",
      "40                 stanfordnlp/sst2             greedy_50_baseline   \n",
      "41                 stanfordnlp/sst2     greedy_50_calib_every_step   \n",
      "42                 stanfordnlp/sst2           greedy_50_calib_once   \n",
      "43                 stanfordnlp/sst2           greedy_50_post_calib   \n",
      "44                 stanfordnlp/sst2        greedy_50_temp_baseline   \n",
      "45                 stanfordnlp/sst2       greedy_unique_5_baseline   \n",
      "46                 stanfordnlp/sst2     greedy_unique_5_post_calib   \n",
      "47                 stanfordnlp/sst2  greedy_unique_5_temp_baseline   \n",
      "\n",
      "      best_ensemble_type      mean  mean_best  significant  \n",
      "0   greedy_50_post_calib  0.950308   0.955379         True  \n",
      "1   greedy_50_post_calib  0.954412   0.955379         True  \n",
      "2   greedy_50_post_calib  0.953065   0.955379         True  \n",
      "3   greedy_50_post_calib  0.955379   0.955379        False  \n",
      "4   greedy_50_post_calib  0.950310   0.955379         True  \n",
      "5   greedy_50_post_calib  0.940640   0.955379         True  \n",
      "6   greedy_50_post_calib  0.949974   0.955379         True  \n",
      "7   greedy_50_post_calib  0.940677   0.955379         True  \n",
      "8   greedy_50_calib_once  0.974827   0.978285         True  \n",
      "9   greedy_50_calib_once  0.978146   0.978285        False  \n",
      "10  greedy_50_calib_once  0.978285   0.978285        False  \n",
      "11  greedy_50_calib_once  0.977954   0.978285        False  \n",
      "12  greedy_50_calib_once  0.974817   0.978285         True  \n",
      "13  greedy_50_calib_once  0.976916   0.978285         True  \n",
      "14  greedy_50_calib_once  0.977910   0.978285        False  \n",
      "15  greedy_50_calib_once  0.976920   0.978285         True  \n",
      "16  greedy_50_calib_once  0.989328   0.989623         True  \n",
      "17  greedy_50_calib_once  0.989619   0.989623        False  \n",
      "18  greedy_50_calib_once  0.989623   0.989623        False  \n",
      "19  greedy_50_calib_once  0.989583   0.989623        False  \n",
      "20  greedy_50_calib_once  0.989331   0.989623         True  \n",
      "21  greedy_50_calib_once  0.989531   0.989623        False  \n",
      "22  greedy_50_calib_once  0.989497   0.989623         True  \n",
      "23  greedy_50_calib_once  0.989526   0.989623        False  \n",
      "24  greedy_50_post_calib  0.982113   0.982176        False  \n",
      "25  greedy_50_post_calib  0.981453   0.982176        False  \n",
      "26  greedy_50_post_calib  0.981886   0.982176        False  \n",
      "27  greedy_50_post_calib  0.982176   0.982176        False  \n",
      "28  greedy_50_post_calib  0.982113   0.982176        False  \n",
      "29  greedy_50_post_calib  0.980918   0.982176         True  \n",
      "30  greedy_50_post_calib  0.981933   0.982176        False  \n",
      "31  greedy_50_post_calib  0.980918   0.982176         True  \n",
      "32  greedy_50_post_calib  0.909139   0.909879        False  \n",
      "33  greedy_50_post_calib  0.909724   0.909879        False  \n",
      "34  greedy_50_post_calib  0.909440   0.909879        False  \n",
      "35  greedy_50_post_calib  0.909879   0.909879        False  \n",
      "36  greedy_50_post_calib  0.909131   0.909879        False  \n",
      "37  greedy_50_post_calib  0.898212   0.909879         True  \n",
      "38  greedy_50_post_calib  0.902505   0.909879         True  \n",
      "39  greedy_50_post_calib  0.898103   0.909879         True  \n",
      "40  greedy_50_post_calib  0.982173   0.982812        False  \n",
      "41  greedy_50_post_calib  0.982121   0.982812        False  \n",
      "42  greedy_50_post_calib  0.981950   0.982812         True  \n",
      "43  greedy_50_post_calib  0.982812   0.982812        False  \n",
      "44  greedy_50_post_calib  0.982173   0.982812        False  \n",
      "45  greedy_50_post_calib  0.978984   0.982812         True  \n",
      "46  greedy_50_post_calib  0.981706   0.982812         True  \n",
      "47  greedy_50_post_calib  0.978985   0.982812         True  \n",
      "\n",
      "Significance testing completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Perform significance tests for AURAC metric\n",
    "\n",
    "# For FTC dataset\n",
    "print(\"=== FTC AURAC Significance Tests ===\")\n",
    "df_ftc_significance_aurac = compare_to_best(\n",
    "    df_ftc_aurac_ci, \n",
    "    metric='aurac', \n",
    "    calibration_method='pure_logits', \n",
    "    alpha=0.05\n",
    ")\n",
    "\n",
    "print(\"FTC AURAC significance results:\")\n",
    "print(df_ftc_significance_aurac[['dataset', 'ensemble_type', 'best_ensemble_type', 'mean', 'mean_best', 'significant']])\n",
    "\n",
    "# For Mini dataset  \n",
    "print(\"\\n=== Mini AURAC Significance Tests ===\")\n",
    "df_mini_significance_aurac = compare_to_best(\n",
    "    df_mini_aurac_ci,\n",
    "    metric='aurac',\n",
    "    calibration_method='pure_logits', \n",
    "    alpha=0.05\n",
    ")\n",
    "\n",
    "print(\"Mini AURAC significance results:\")\n",
    "print(df_mini_significance_aurac[['dataset', 'ensemble_type', 'best_ensemble_type', 'mean', 'mean_best', 'significant']])\n",
    "\n",
    "print(\"\\nSignificance testing completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "oyxs6maplx",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add LaTeX table formatting functions (copied from main tables notebook)\n",
    "\n",
    "def parse_mean_ci(value_str: str) -> float:\n",
    "    \"\"\"\n",
    "    Extract mean value from 'mean ± CI' string.\n",
    "    \n",
    "    Args:\n",
    "        value_str: String in format \"0.1234 ± 0.0056\"\n",
    "    \n",
    "    Returns:\n",
    "        float: The mean value\n",
    "    \"\"\"\n",
    "    if pd.isna(value_str) or value_str == 'nan':\n",
    "        return float('nan')\n",
    "    return float(value_str.split(' ± ')[0])\n",
    "\n",
    "def format_cell_value(value_str: str, is_best: bool, is_not_significant: bool) -> str:\n",
    "    \"\"\"\n",
    "    Apply LaTeX formatting to a cell value.\n",
    "    \n",
    "    Args:\n",
    "        value_str: Original value string \"mean ± CI\"\n",
    "        is_best: True if this is the best value in the column\n",
    "        is_not_significant: True if not significantly different from best\n",
    "    \n",
    "    Returns:\n",
    "        str: LaTeX formatted string\n",
    "    \"\"\"\n",
    "    if pd.isna(value_str) or value_str == 'nan':\n",
    "        return ''\n",
    "    \n",
    "    # Start with the original value\n",
    "    formatted = value_str\n",
    "    \n",
    "    # Apply bold if it's the best value\n",
    "    if is_best:\n",
    "        formatted = f\"\\\\textbf{{{formatted}}}\"\n",
    "    \n",
    "    # Apply shading if it's not significantly different from best OR if it is the best\n",
    "    if is_not_significant or is_best:\n",
    "        formatted = f\"\\\\cellcolor{{gray!20}}{formatted}\"\n",
    "    \n",
    "    return formatted\n",
    "\n",
    "def get_significance_for_dataset(df_significance: pd.DataFrame, \n",
    "                                dataset: str, \n",
    "                                ensemble_type: str) -> tuple[bool, bool]:\n",
    "    \"\"\"\n",
    "    Get significance information for a specific dataset-ensemble combination.\n",
    "    \n",
    "    Args:\n",
    "        df_significance: DataFrame from compare_to_best function\n",
    "        dataset: Dataset name (e.g., 'DBpedia', 'News')\n",
    "        ensemble_type: Ensemble type name (ORIGINAL name, not display name)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (is_best, is_not_significant)\n",
    "    \"\"\"\n",
    "    # Map short dataset names back to full names for lookup\n",
    "    dataset_map_reverse = {\n",
    "        'DBpedia': 'dbpedia_14',\n",
    "        'News': 'ag_news', \n",
    "        'SST-2': 'stanfordnlp/sst2',\n",
    "        'SetFit': 'SetFit/mnli',\n",
    "        'Tweet': 'mteb/tweet_sentiment_extraction',\n",
    "        'IMDB': 'imdb'\n",
    "    }\n",
    "    \n",
    "    full_dataset_name = dataset_map_reverse.get(dataset, dataset)\n",
    "    \n",
    "    # Find the row for this dataset and ensemble type\n",
    "    mask = (df_significance['dataset'] == full_dataset_name) & \\\n",
    "           (df_significance['ensemble_type'] == ensemble_type)\n",
    "    \n",
    "    if not mask.any():\n",
    "        return False, False\n",
    "    \n",
    "    row = df_significance[mask].iloc[0]\n",
    "    \n",
    "    # Check if this ensemble type is the best\n",
    "    is_best = (row['ensemble_type'] == row['best_ensemble_type'])\n",
    "    \n",
    "    # Check if it's not significantly different from best\n",
    "    is_not_significant = not row['significant']\n",
    "    \n",
    "    return is_best, is_not_significant\n",
    "\n",
    "def create_latex_table_with_significance(\n",
    "    df_wide: pd.DataFrame,           # Wide-format data from create_report_view_df\n",
    "    df_significance: pd.DataFrame,   # Significance test results\n",
    "    ensemble_type_mapping: dict,     # Original -> Display name mapping\n",
    "    metric_name: str,                # 'NLL', 'AUC', 'AURAC', 'Set Size'\n",
    "    dataset_name: str,               # 'FTC-metadataset', 'Extended', 'Mini'\n",
    "    caption: str,                    # Full caption text\n",
    "    label: str,                      # e.g., 'tab:nll_pure_logits'\n",
    "    maximize: bool = False           # True for AUC/AURAC, False for NLL/Set Size\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Create a publication-ready LaTeX table with statistical significance formatting.\n",
    "    \n",
    "    Args:\n",
    "        df_wide: Wide-format DataFrame with ensemble_type as rows, datasets as columns\n",
    "        df_significance: Results from compare_to_best() function\n",
    "        ensemble_type_mapping: Mapping from original names to display names\n",
    "        metric_name: Name of the metric for the table\n",
    "        dataset_name: Dataset name for caption\n",
    "        caption: Full LaTeX caption\n",
    "        label: LaTeX label for referencing\n",
    "        maximize: True if higher values are better (AUC, AURAC), False otherwise\n",
    "    \n",
    "    Returns:\n",
    "        str: Complete LaTeX table code\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_formatted = df_wide.copy()\n",
    "    \n",
    "    # Handle metric name mapping - 'AUROC' should map to 'AUC' in the data\n",
    "    metric_lookup = {'AUROC': 'AUC', 'Set_Size': 'Set Size'}\n",
    "    data_metric_name = metric_lookup.get(metric_name, metric_name)\n",
    "    \n",
    "    # Get the metric column names (should be tuples like (metric, dataset))\n",
    "    metric_cols = [col for col in df_formatted.columns if isinstance(col, tuple) and col[0] == data_metric_name]\n",
    "    \n",
    "    if not metric_cols:\n",
    "        raise ValueError(f\"No columns found for metric '{data_metric_name}' (original: '{metric_name}') in the DataFrame\")\n",
    "    \n",
    "    # Create reverse mapping from display names to original names for significance lookup\n",
    "    reverse_mapping = {v: k for k, v in ensemble_type_mapping.items()}\n",
    "    \n",
    "    # Process each dataset column\n",
    "    for metric, dataset in metric_cols:\n",
    "        col_name = (metric, dataset)\n",
    "        \n",
    "        # Apply formatting to each cell in this column\n",
    "        for row_idx in range(len(df_formatted)):\n",
    "            # Handle the ensemble_type column access properly with MultiIndex\n",
    "            ensemble_display_name = df_formatted.iloc[row_idx]['ensemble_type']\n",
    "            if isinstance(ensemble_display_name, pd.Series):\n",
    "                ensemble_display_name = ensemble_display_name.iloc[0]\n",
    "            \n",
    "            ensemble_original_name = reverse_mapping.get(ensemble_display_name, ensemble_display_name)\n",
    "            \n",
    "            # Get the original value using iloc for MultiIndex\n",
    "            original_value = df_formatted.iloc[row_idx][col_name]\n",
    "            if isinstance(original_value, pd.Series):\n",
    "                original_value = original_value.iloc[0]\n",
    "            \n",
    "            if pd.isna(original_value) or str(original_value) == 'nan':\n",
    "                # Set empty string using loc for MultiIndex\n",
    "                df_formatted.loc[row_idx, col_name] = ''\n",
    "                continue\n",
    "            \n",
    "            # Get significance information  \n",
    "            is_best, is_not_significant = get_significance_for_dataset(\n",
    "                df_significance, dataset, ensemble_original_name\n",
    "            )\n",
    "            \n",
    "            # Apply formatting\n",
    "            formatted_value = format_cell_value(str(original_value), is_best, is_not_significant)\n",
    "            # Set value using loc for MultiIndex\n",
    "            df_formatted.loc[row_idx, col_name] = formatted_value\n",
    "    \n",
    "    # Build the LaTeX table structure\n",
    "    datasets = [col[1] for col in metric_cols]  # Extract dataset names\n",
    "    n_datasets = len(datasets)\n",
    "    \n",
    "    # Column specification: l for ensemble type + c for each dataset\n",
    "    col_spec = 'l' + 'c' * n_datasets\n",
    "    \n",
    "    # Build header row\n",
    "    header = ' & '.join(['Ensemble Type'] + datasets) + ' \\\\\\\\'\n",
    "    \n",
    "    # Build data rows\n",
    "    data_rows = []\n",
    "    for row_idx in range(len(df_formatted)):\n",
    "        ensemble_name = df_formatted.iloc[row_idx]['ensemble_type']\n",
    "        if isinstance(ensemble_name, pd.Series):\n",
    "            ensemble_name = ensemble_name.iloc[0]\n",
    "        \n",
    "        values = []\n",
    "        for col in metric_cols:\n",
    "            value = df_formatted.iloc[row_idx][col]\n",
    "            if isinstance(value, pd.Series):\n",
    "                value = value.iloc[0]\n",
    "            values.append(str(value))\n",
    "        \n",
    "        row_str = ' & '.join([str(ensemble_name)] + values) + ' \\\\\\\\'\n",
    "        data_rows.append(row_str)\n",
    "    \n",
    "    # Create separators for different ensemble groups if needed\n",
    "    formatted_rows = []\n",
    "    for i, row_str in enumerate(data_rows):\n",
    "        if i > 0 and ('G50' in row_str and 'G5' in data_rows[i-1] and 'G50' not in data_rows[i-1]):\n",
    "            # Add midrule before first G50 row\n",
    "            formatted_rows.append('\\\\midrule')\n",
    "        formatted_rows.append(row_str)\n",
    "    \n",
    "    # Combine everything into the full LaTeX table\n",
    "    latex_lines = [\n",
    "        '\\\\begin{table}[h]',\n",
    "        f'  \\\\caption{{{caption}}}',\n",
    "        f'  \\\\label{{{label}}}',\n",
    "        '  \\\\centering',\n",
    "        '  \\\\resizebox{\\\\textwidth}{!}{%',\n",
    "        f'  \\\\begin{{tabular}}{{{col_spec}}}',\n",
    "        '    \\\\toprule',\n",
    "        f'    {header}',\n",
    "        '    \\\\midrule'\n",
    "    ]\n",
    "    \n",
    "    # Add data rows\n",
    "    for row in formatted_rows:\n",
    "        latex_lines.append(f'    {row}')\n",
    "    \n",
    "    # Close the table\n",
    "    latex_lines.extend([\n",
    "        '    \\\\bottomrule',\n",
    "        '  \\\\end{tabular}',\n",
    "        '  }',\n",
    "        '\\\\end{table}'\n",
    "    ])\n",
    "    \n",
    "    # Return with proper newlines\n",
    "    return '\\n'.join(latex_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c565e913",
   "metadata": {},
   "source": [
    "## AURAC for Full Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fefcdbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FTC (Full Dataset) AURAC TABLE ===\n",
      "\\begin{table}[h]\n",
      "  \\caption{Full dataset: Area Under the Rejection-Accuracy Curve (AURAC) over data splits; mean ± 95\\% confidence interval half-width) on the full dataset (100\\%). The best mean is shown in bold, and methods not significantly different from the best (paired test, \\( \\alpha = 0.05 \\)) are shaded.}\n",
      "  \\label{tab:aurac_ftc_pure_logits}\n",
      "  \\centering\n",
      "  \\resizebox{\\textwidth}{!}{%\n",
      "  \\begin{tabular}{lcccccc}\n",
      "    \\toprule\n",
      "    Ensemble Type & DBpedia & News & SST-2 & SetFit & Tweet & IMDB \\\\\n",
      "    \\midrule\n",
      "    G5 & 0.9895 ± 0.0 & 0.981 ± 0.0011 & 0.984 ± 0.0005 & 0.8915 ± 0.0008 & 0.9103 ± 0.0028 & \\cellcolor{gray!20}\\textbf{0.9859 ± 0.0002} \\\\\n",
      "    G5 p.t.c. & 0.9895 ± 0.0 & 0.981 ± 0.0011 & 0.984 ± 0.0005 & 0.8915 ± 0.0008 & 0.9103 ± 0.0027 & \\cellcolor{gray!20}0.9859 ± 0.0002 \\\\\n",
      "    G5 JUCAL & \\cellcolor{gray!20}0.9897 ± 0.0 & \\cellcolor{gray!20}0.9829 ± 0.0005 & 0.9842 ± 0.0005 & 0.924 ± 0.0006 & 0.9211 ± 0.0006 & \\cellcolor{gray!20}0.9858 ± 0.0002 \\\\\n",
      "    \\midrule\n",
      "    G50 & 0.9895 ± 0.0 & 0.981 ± 0.0008 & 0.9833 ± 0.0005 & 0.9023 ± 0.0006 & 0.9157 ± 0.0021 & 0.9838 ± 0.0003 \\\\\n",
      "    G50 p.t.c. & 0.9895 ± 0.0 & 0.981 ± 0.0008 & 0.9833 ± 0.0005 & 0.9023 ± 0.0006 & 0.9158 ± 0.0021 & 0.9838 ± 0.0003 \\\\\n",
      "    G50 JUCAL & \\cellcolor{gray!20}\\textbf{0.9897 ± 0.0} & \\cellcolor{gray!20}0.9835 ± 0.0005 & \\cellcolor{gray!20}0.9849 ± 0.0005 & 0.9237 ± 0.0007 & \\cellcolor{gray!20}0.9236 ± 0.0014 & \\cellcolor{gray!20}0.9855 ± 0.0002 \\\\\n",
      "    G50 r.c.o. JUCAL & \\cellcolor{gray!20}0.9897 ± 0.0 & \\cellcolor{gray!20}\\textbf{0.9837 ± 0.0005} & \\cellcolor{gray!20}0.985 ± 0.0004 & \\cellcolor{gray!20}\\textbf{0.9252 ± 0.0005} & \\cellcolor{gray!20}\\textbf{0.9249 ± 0.0013} & \\cellcolor{gray!20}0.9859 ± 0.0002 \\\\\n",
      "    G50 r.c. JUCAL & \\cellcolor{gray!20}0.9897 ± 0.0 & \\cellcolor{gray!20}0.9837 ± 0.0005 & \\cellcolor{gray!20}\\textbf{0.985 ± 0.0005} & 0.9226 ± 0.0005 & \\cellcolor{gray!20}0.9244 ± 0.0015 & \\cellcolor{gray!20}0.9859 ± 0.0002 \\\\\n",
      "    \\bottomrule\n",
      "  \\end{tabular}\n",
      "  }\n",
      "\\end{table}\n",
      "\n",
      "✅ FTC AURAC table saved to: LATEX/llm/ftc_aurac_formatted.tex\n",
      "Table shape: (8, 8)\n"
     ]
    }
   ],
   "source": [
    "# Generate properly formatted FTC AURAC table with statistical significance\n",
    "\n",
    "# Define ensemble type mapping for FTC data\n",
    "etype_map_ftc = {\n",
    "    'greedy_unique_5_baseline': 'G5',\n",
    "    'greedy_unique_5_temp_baseline': 'G5 p.t.c.',\n",
    "    'greedy_unique_5_post_calib': 'G5 JUCAL',\n",
    "    'greedy_50_baseline': 'G50',\n",
    "    'greedy_50_temp_baseline': 'G50 p.t.c.',\n",
    "    'greedy_50_post_calib': 'G50 JUCAL',\n",
    "    'greedy_50_calib_once': 'G50 r.c.o. JUCAL',\n",
    "    'greedy_50_calib_every_step': 'G50 r.c. JUCAL'\n",
    "}\n",
    "\n",
    "# Define proper ensemble ordering for FTC data\n",
    "ensemble_order_ftc = [\n",
    "    'G5', 'G5 p.t.c.', 'G5 JUCAL',\n",
    "    'G50', 'G50 p.t.c.', 'G50 JUCAL', \n",
    "    'G50 r.c.o. JUCAL', 'G50 r.c. JUCAL'\n",
    "]\n",
    "\n",
    "# Apply ensemble mapping to data\n",
    "df_ftc_aurac_subset_mapped = df_ftc_aurac_subset.copy()\n",
    "for original, mapped in etype_map_ftc.items():\n",
    "    df_ftc_aurac_subset_mapped['ensemble_type'] = df_ftc_aurac_subset_mapped['ensemble_type'].replace(original, mapped)\n",
    "\n",
    "# Create FTC AURAC wide-format table\n",
    "df_ftc_report_aurac = create_report_view_df(\n",
    "    df_ftc_aurac_subset_mapped,\n",
    "    value_vars=['aurac_mean±CI'], \n",
    "    calibration_method='pure_logits',\n",
    "    custom_rows=ensemble_order_ftc\n",
    ")\n",
    "\n",
    "# Generate caption\n",
    "caption = (\"Full dataset: Area Under the Rejection-Accuracy Curve (AURAC) over data splits; \"\n",
    "          \"mean ± 95\\\\% confidence interval half-width) on the full dataset (100\\\\%). \"\n",
    "          \"The best mean is shown in bold, and methods not significantly different from the best \"\n",
    "          \"(paired test, \\\\( \\\\alpha = 0.05 \\\\)) are shaded.\")\n",
    "\n",
    "# Generate LaTeX table with significance formatting\n",
    "latex_table_ftc = create_latex_table_with_significance(\n",
    "    df_wide=df_ftc_report_aurac,\n",
    "    df_significance=df_ftc_significance_aurac,\n",
    "    ensemble_type_mapping=etype_map_ftc,\n",
    "    metric_name='AURAC',\n",
    "    dataset_name='Full',\n",
    "    caption=caption,\n",
    "    label='tab:aurac_ftc_pure_logits',\n",
    "    maximize=True\n",
    ")\n",
    "\n",
    "# Save to file\n",
    "latex_path = Path('LATEX/llm')\n",
    "latex_path.mkdir(parents=True, exist_ok=True)\n",
    "filename_ftc = 'ftc_aurac_formatted.tex'\n",
    "filepath_ftc = latex_path / filename_ftc\n",
    "\n",
    "with open(filepath_ftc, 'w') as f:\n",
    "    f.write(latex_table_ftc)\n",
    "\n",
    "print(\"=== FTC (Full Dataset) AURAC TABLE ===\")\n",
    "print(latex_table_ftc)\n",
    "print(f\"\\n✅ FTC AURAC table saved to: {filepath_ftc}\")\n",
    "print(f\"Table shape: {df_ftc_report_aurac.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159e4da1",
   "metadata": {},
   "source": [
    "## Aurac for Mini Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4badae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MINI DATASET AURAC TABLE ===\n",
      "\\begin{table}[h]\n",
      "  \\caption{Mini dataset: Area Under the Rejection-Accuracy Curve (AURAC) over data splits; mean ± 95\\% confidence interval half-width) on the full dataset (100\\%). The best mean is shown in bold, and methods not significantly different from the best (paired test, \\( \\alpha = 0.05 \\)) are shaded.}\n",
      "  \\label{tab:aurac_mini_pure_logits}\n",
      "  \\centering\n",
      "  \\resizebox{\\textwidth}{!}{%\n",
      "  \\begin{tabular}{lcccccc}\n",
      "    \\toprule\n",
      "    Ensemble Type & DBpedia & News & SST-2 & SetFit & Tweet & IMDB \\\\\n",
      "    \\midrule\n",
      "    G5 & \\cellcolor{gray!20}0.9895 ± 0.0001 & 0.9769 ± 0.0002 & 0.979 ± 0.0008 & 0.9406 ± 0.0005 & 0.8982 ± 0.0026 & 0.9809 ± 0.0006 \\\\\n",
      "    G5 p.t.c. & \\cellcolor{gray!20}0.9895 ± 0.0001 & 0.9769 ± 0.0003 & 0.979 ± 0.0008 & 0.9407 ± 0.0005 & 0.8981 ± 0.0026 & 0.9809 ± 0.0006 \\\\\n",
      "    G5 JUCAL & 0.9895 ± 0.0001 & \\cellcolor{gray!20}0.9779 ± 0.0003 & 0.9817 ± 0.0004 & 0.95 ± 0.0005 & 0.9025 ± 0.0018 & \\cellcolor{gray!20}0.9819 ± 0.0005 \\\\\n",
      "    \\midrule\n",
      "    G50 & 0.9893 ± 0.0001 & 0.9748 ± 0.0005 & \\cellcolor{gray!20}0.9822 ± 0.0005 & 0.9503 ± 0.0003 & \\cellcolor{gray!20}0.9091 ± 0.0018 & \\cellcolor{gray!20}0.9821 ± 0.0004 \\\\\n",
      "    G50 p.t.c. & 0.9893 ± 0.0001 & 0.9748 ± 0.0005 & \\cellcolor{gray!20}0.9822 ± 0.0005 & 0.9503 ± 0.0003 & \\cellcolor{gray!20}0.9091 ± 0.0018 & \\cellcolor{gray!20}0.9821 ± 0.0004 \\\\\n",
      "    G50 JUCAL & \\cellcolor{gray!20}0.9896 ± 0.0 & \\cellcolor{gray!20}0.978 ± 0.0005 & \\cellcolor{gray!20}\\textbf{0.9828 ± 0.0005} & \\cellcolor{gray!20}\\textbf{0.9554 ± 0.0002} & \\cellcolor{gray!20}\\textbf{0.9099 ± 0.0017} & \\cellcolor{gray!20}\\textbf{0.9822 ± 0.0005} \\\\\n",
      "    G50 r.c.o. JUCAL & \\cellcolor{gray!20}\\textbf{0.9896 ± 0.0001} & \\cellcolor{gray!20}\\textbf{0.9783 ± 0.0006} & 0.982 ± 0.0004 & 0.9531 ± 0.0003 & \\cellcolor{gray!20}0.9094 ± 0.002 & \\cellcolor{gray!20}0.9819 ± 0.0003 \\\\\n",
      "    G50 r.c. JUCAL & \\cellcolor{gray!20}0.9896 ± 0.0001 & \\cellcolor{gray!20}0.9781 ± 0.0006 & \\cellcolor{gray!20}0.9821 ± 0.0002 & 0.9544 ± 0.0002 & \\cellcolor{gray!20}0.9097 ± 0.0014 & \\cellcolor{gray!20}0.9815 ± 0.0006 \\\\\n",
      "    \\bottomrule\n",
      "  \\end{tabular}\n",
      "  }\n",
      "\\end{table}\n",
      "\n",
      "✅ Mini AURAC table saved to: LATEX/llm/mini_aurac_formatted.tex\n",
      "Table shape: (8, 8)\n",
      "\n",
      "============================================================\n",
      "FINAL SUMMARY\n",
      "============================================================\n",
      "✅ Both AURAC tables generated with proper LaTeX formatting!\n",
      "✅ Statistical significance formatting applied (bold + shading)\n",
      "✅ Tables ready for your ICLR paper\n",
      "\n",
      "Generated files:\n",
      "  - LATEX/llm/ftc_aurac_formatted.tex\n",
      "  - LATEX/llm/mini_aurac_formatted.tex\n",
      "\n",
      "Features:\n",
      "  • Bold formatting for best values\n",
      "  • Gray shading for non-significantly different values\n",
      "  • Proper LaTeX table structure with captions and labels\n",
      "  • Single backslashes (correct LaTeX formatting)\n",
      "  • 'Full dataset' and 'Mini dataset' in captions\n"
     ]
    }
   ],
   "source": [
    "# Generate properly formatted Mini AURAC table with statistical significance\n",
    "\n",
    "# Apply ensemble mapping to Mini data (same mapping as FTC)\n",
    "df_mini_aurac_subset_mapped = df_mini_aurac_subset.copy()\n",
    "for original, mapped in etype_map_ftc.items():\n",
    "    df_mini_aurac_subset_mapped['ensemble_type'] = df_mini_aurac_subset_mapped['ensemble_type'].replace(original, mapped)\n",
    "\n",
    "# Create Mini AURAC wide-format table\n",
    "df_mini_report_aurac = create_report_view_df(\n",
    "    df_mini_aurac_subset_mapped,\n",
    "    value_vars=['aurac_mean±CI'], \n",
    "    calibration_method='pure_logits',\n",
    "    custom_rows=ensemble_order_ftc\n",
    ")\n",
    "\n",
    "# Generate caption for Mini dataset\n",
    "caption_mini = (\"Mini dataset: Area Under the Rejection-Accuracy Curve (AURAC) over data splits; \"\n",
    "               \"mean ± 95\\\\% confidence interval half-width) on the full dataset (100\\\\%). \"\n",
    "               \"The best mean is shown in bold, and methods not significantly different from the best \"\n",
    "               \"(paired test, \\\\( \\\\alpha = 0.05 \\\\)) are shaded.\")\n",
    "\n",
    "# Generate LaTeX table with significance formatting\n",
    "latex_table_mini = create_latex_table_with_significance(\n",
    "    df_wide=df_mini_report_aurac,\n",
    "    df_significance=df_mini_significance_aurac,\n",
    "    ensemble_type_mapping=etype_map_ftc,\n",
    "    metric_name='AURAC',\n",
    "    dataset_name='Mini',\n",
    "    caption=caption_mini,\n",
    "    label='tab:aurac_mini_pure_logits',\n",
    "    maximize=True\n",
    ")\n",
    "\n",
    "# Save to file\n",
    "filename_mini = 'mini_aurac_formatted.tex'\n",
    "filepath_mini = latex_path / filename_mini\n",
    "\n",
    "with open(filepath_mini, 'w') as f:\n",
    "    f.write(latex_table_mini)\n",
    "\n",
    "print(\"=== MINI DATASET AURAC TABLE ===\")\n",
    "print(latex_table_mini)\n",
    "print(f\"\\n✅ Mini AURAC table saved to: {filepath_mini}\")\n",
    "print(f\"Table shape: {df_mini_report_aurac.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(\"✅ Both AURAC tables generated with proper LaTeX formatting!\")\n",
    "print(\"✅ Statistical significance formatting applied (bold + shading)\")\n",
    "print(\"✅ Tables ready for your ICLR paper\")\n",
    "print(f\"\\nGenerated files:\")\n",
    "print(f\"  - {filepath_ftc}\")\n",
    "print(f\"  - {filepath_mini}\")\n",
    "print(f\"\\nFeatures:\")\n",
    "print(\"  • Bold formatting for best values\")\n",
    "print(\"  • Gray shading for non-significantly different values\")\n",
    "print(\"  • Proper LaTeX table structure with captions and labels\")\n",
    "print(\"  • Single backslashes (correct LaTeX formatting)\")\n",
    "print(\"  • 'Full dataset' and 'Mini dataset' in captions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3016e6c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
