{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "388c2f9d",
   "metadata": {},
   "source": [
    "# LLM Analysis Tables\n",
    "\n",
    "This notebook generates LaTeX tables from pre-computed metrics for the LLM uncertainty analysis.\n",
    "\n",
    "**Prerequisites:** Run `iclr_llm_analysis_plots_v2.ipynb` first to compute metrics.\n",
    "\n",
    "**Contents:**\n",
    "1. Load pre-computed metrics from plots_v2 notebook\n",
    "2. Generate significance tests \n",
    "3. Create LaTeX tables for all metrics (NLL, AUC, AURAC, Set Size)\n",
    "4. Special case: Compute AURAC for older April 2025 data (not in plots_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a38bd9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ttest_ind_from_stats, norm\n",
    "\n",
    "sys.path.append('../src/')\n",
    "from plotting import get_coverage_threshold_and_size, get_auc, compute_aurac"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54dc1e7",
   "metadata": {},
   "source": [
    "## Helper Functions for Table Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "266b11bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path('Ensembling_Finetuned_LLMs')\n",
    "\n",
    "def read_file(file_path, base_path=base_path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read a file and return a DataFrame.\n",
    "    \"\"\"\n",
    "    path  = base_path / 'llm_experiments_data' / file_path\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File {path} does not exist.\")\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def combine_and_clean_dataframes(df1, df2 = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Combine and clean the dataframes.\n",
    "    \"\"\"\n",
    "    parts = [df1]\n",
    "    if df2 is not None:\n",
    "        parts.append(df2)\n",
    "    # Combine the dataframes\n",
    "    combined_df = pd.concat(parts, ignore_index=True)\n",
    "    combined_df = combined_df.drop_duplicates(subset=['dataset', 'seed', 'method', 'ensemble_type'], keep='first')\n",
    "    combined_df.reset_index(drop=True, inplace=True)\n",
    "    return combined_df\n",
    "\n",
    "def calc_ci_for_df(df) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Code assumes the right columns are present in the dataframe\n",
    "    \"\"\"\n",
    "    #group by dataset, method, ensemble_type and calculate mean, std, count\n",
    "    df = df.groupby(['dataset', 'method', 'ensemble_type'])[\n",
    "    ['ensemble_size', 'ensemble_unique_size', 'nll_test', 'c1', 'c2', 'epi_scalar', 'threshold', 'set_size', 'auc', 'aurac', 'aorac' ]\n",
    "    ].agg(\n",
    "        ['mean', 'std', 'count']\n",
    "        ).reset_index()\n",
    "    # note we will get a double index\n",
    "    columns = ['ensemble_size', 'ensemble_unique_size', 'nll_test', 'c1', 'c2', 'epi_scalar', 'threshold', 'set_size', 'auc', 'aurac', 'aorac' ]\n",
    "    for col in columns:\n",
    "        df[(col, 'CI')] = 1.96 * (df[(col, 'std')] / np.sqrt(df[(col, 'count')].replace(0, np.nan)))\n",
    "        df[(col,'mean±CI')] = (df[(col, 'mean')].round(4).astype(str) + \n",
    "                              \" ± \" + df[(col, 'CI')].round(4).astype(str))\n",
    "    return df\n",
    "\n",
    "def flatten_subset_df(df, subset = ['dataset', 'method', 'ensemble_type', 'nll_test_mean±CI',\n",
    "                                     'threshold_mean±CI', 'set_size_mean±CI', 'auc_mean±CI', 'aurac_mean±CI', 'aorac_mean±CI']) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Flatten the dataframe\n",
    "    \"\"\"\n",
    "    # Flatten the multi-index columns\n",
    "    df.columns = [\n",
    "                '_'.join(col).strip('_') if col[1] else col[0] \n",
    "                  for col in df.columns.values]\n",
    "    #return subset of columns\n",
    "    return df[subset]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20ab9bd",
   "metadata": {},
   "source": [
    "## Functions for determining statistical significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0cedf0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_to_best(\n",
    "    df: pd.DataFrame,\n",
    "    metric: str,\n",
    "    ensemble_types: list = None,  # Auto-detect if None\n",
    "    calibration_method: str = 'pure_logits',\n",
    "    alpha: float = 0.05\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each dataset, find the best ensemble_type on `metric`,\n",
    "    then compare *all* ensemble_types to that best one.\n",
    "\n",
    "    Args:\n",
    "      df                 : DataFrame with columns\n",
    "                           ['dataset','ensemble_type','method',\n",
    "                            f'{metric}_mean', f'{metric}_std', f'{metric}_count', …]\n",
    "      ensemble_types     : list of ensemble_type values to compare (auto-detect if None)\n",
    "      calibration_method : the `method` value to filter on (e.g. 'pure_logits')\n",
    "      metric             : one of ['nll_test','threshold','set_size','auc','aurac','aorac']\n",
    "      alpha              : for 95% CI and p<alpha test\n",
    "\n",
    "    Returns:\n",
    "      DataFrame with one row per (dataset, ensemble_type) containing:\n",
    "        • best_ensemble_type  \n",
    "        • mean & CI for that row & for the best  \n",
    "        • ci_no_overlap (bool)  \n",
    "        • significant (bool, p<alpha)  \n",
    "        • p_value, t_stat\n",
    "    \"\"\"\n",
    "    z = norm.ppf(1 - alpha/2)  # ≈1.96 for alpha=0.05\n",
    "    print(f\"Using z={z:.2f} for alpha={alpha:.2f}\")\n",
    "    \n",
    "    # Auto-detect ensemble types if not provided\n",
    "    if ensemble_types is None:\n",
    "        ensemble_types = df['ensemble_type'].unique().tolist()\n",
    "        print(f\"Auto-detected ensemble types: {ensemble_types}\")\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    for ds in df['dataset'].unique():\n",
    "        # restrict to this dataset, this calibration method, and only those ensemble_types\n",
    "        sub = df[\n",
    "            (df['dataset'] == ds) &\n",
    "            (df['method']  == calibration_method) &\n",
    "            (df['ensemble_type'].isin(ensemble_types))\n",
    "        ].copy()\n",
    "\n",
    "        if sub.empty:\n",
    "            print(f\"Warning: No data for dataset {ds} with method {calibration_method} and ensemble types {ensemble_types}\")\n",
    "            continue\n",
    "\n",
    "        # compute 95% CI boundaries for every row\n",
    "        sub[f'{metric}_ci_lo'] = sub[f'{metric}_mean'] - z * sub[f'{metric}_std'] / np.sqrt(sub[f'{metric}_count'])\n",
    "        sub[f'{metric}_ci_hi'] = sub[f'{metric}_mean'] + z * sub[f'{metric}_std'] / np.sqrt(sub[f'{metric}_count'])\n",
    "\n",
    "        # pick the best row - for AURAC we want max, for AORAC we want min\n",
    "        if metric in ['auc', 'aurac']:\n",
    "            best_idx = sub[f'{metric}_mean'].idxmax()\n",
    "        else:\n",
    "            best_idx = sub[f'{metric}_mean'].idxmin()\n",
    "        best = sub.loc[best_idx]\n",
    "        print(f\"{best['ensemble_type']} is the best ensemble type for {ds} with {metric}={best[metric+'_mean']:.4f} \")\n",
    "        \n",
    "        # compare every row to best\n",
    "        for _, row in sub.iterrows():\n",
    "            m1, s1, n1 = row[f'{metric}_mean'], row[f'{metric}_std'], row[f'{metric}_count']\n",
    "            lo1, hi1 = row[f'{metric}_ci_lo'], row[f'{metric}_ci_hi']\n",
    "\n",
    "            m0, s0, n0 = best[f'{metric}_mean'], best[f'{metric}_std'], best[f'{metric}_count']\n",
    "            lo0, hi0 = best[f'{metric}_ci_lo'], best[f'{metric}_ci_hi']\n",
    "\n",
    "            # CI non-overlap?\n",
    "            ci_no_overlap = (hi1 < lo0) or (hi0 < lo1)\n",
    "\n",
    "            # Welch's t-test\n",
    "            t_stat, p_val = ttest_ind_from_stats(\n",
    "                mean1=m1, std1=s1, nobs1=n1,\n",
    "                mean2=m0, std2=s0, nobs2=n0,\n",
    "                equal_var=False\n",
    "            )\n",
    "            significant = (p_val < alpha)\n",
    "\n",
    "            results.append({\n",
    "                'calibration_method': calibration_method,\n",
    "                'metric':              metric,\n",
    "                'ci_no_overlap':       ci_no_overlap,\n",
    "                'hi':                  hi1,\n",
    "                'lo':                  lo1,\n",
    "                'best_ensemble_type':  best['ensemble_type'],\n",
    "                'dataset':             ds,\n",
    "                'mean':                m1,\n",
    "                'mean_best':           m0,\n",
    "                'ensemble_type':       row['ensemble_type'],\n",
    "                'significant':         significant\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def compare_to_two(\n",
    "    df: pd.DataFrame,\n",
    "    metric: str,\n",
    "    ensemble_type_1: str,\n",
    "    ensemble_type_2: str,\n",
    "    calibration_method: str = 'pure_logits',\n",
    "    alpha: float = 0.05\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compare two ensemble types (ensemble_type_1 vs. ensemble_type_2)\n",
    "    on a single metric, for every dataset.\n",
    "\n",
    "    Returns columns:\n",
    "      - dataset\n",
    "      - metric\n",
    "      - ensemble_type_1, ensemble_type_2\n",
    "      - mean_1, ci_lo_1, ci_hi_1\n",
    "      - mean_2, ci_lo_2, ci_hi_2\n",
    "      - ci_no_overlap  (True if CIs are disjoint)\n",
    "      - t_stat, p_value, significant\n",
    "    \"\"\"\n",
    "    # z-value for two-sided (1-alpha) CI\n",
    "    z = norm.ppf(1 - alpha/2)\n",
    "    print(f\"Using z = {z:.2f} for {100*(1-alpha):.0f}% CI\")\n",
    "\n",
    "    results = []\n",
    "    for ds in df['dataset'].unique():\n",
    "        # filter to this dataset, this method, these two ensemble types\n",
    "        sub = df[\n",
    "            (df['dataset'] == ds) &\n",
    "            (df['method']  == calibration_method) &\n",
    "            (df['ensemble_type'].isin([ensemble_type_1, ensemble_type_2]))\n",
    "        ]\n",
    "        if len(sub) < 2:\n",
    "            # skip if we don't have both\n",
    "            continue\n",
    "\n",
    "        # pull them out\n",
    "        row1 = sub[sub['ensemble_type'] == ensemble_type_1].iloc[0]\n",
    "        row2 = sub[sub['ensemble_type'] == ensemble_type_2].iloc[0]\n",
    "\n",
    "        # summary stats\n",
    "        m1, s1, n1 = row1[f\"{metric}_mean\"], row1[f\"{metric}_std\"], row1[f\"{metric}_count\"]\n",
    "        m2, s2, n2 = row2[f\"{metric}_mean\"], row2[f\"{metric}_std\"], row2[f\"{metric}_count\"]\n",
    "\n",
    "        # 95% CIs\n",
    "        lo1, hi1 = m1 - z*s1/np.sqrt(n1), m1 + z*s1/np.sqrt(n1)\n",
    "        lo2, hi2 = m2 - z*s2/np.sqrt(n2), m2 + z*s2/np.sqrt(n2)\n",
    "\n",
    "        # disjoint?\n",
    "        ci_no_overlap = (hi1 < lo2) or (hi2 < lo1)\n",
    "\n",
    "        # Welch's t-test\n",
    "        t_stat, p_val = ttest_ind_from_stats(\n",
    "            mean1=m1, std1=s1, nobs1=n1,\n",
    "            mean2=m2, std2=s2, nobs2=n2,\n",
    "            equal_var=False\n",
    "        )\n",
    "        significant = (p_val < alpha)\n",
    "        # NOTE we again report no overlap => True stronger than significant\n",
    "        results.append({\n",
    "            'dataset'                : ds,\n",
    "            'metric'                 : metric,\n",
    "            f'mean_{ensemble_type_1}': m1,\n",
    "            f'mean_{ensemble_type_2}': m2,\n",
    "            'ci_no_overlap'          : ci_no_overlap,\n",
    "            'significant'            : significant,\n",
    "            f'ci_lo_{ensemble_type_1}': lo1,\n",
    "            f'ci_hi_{ensemble_type_1}': hi1,\n",
    "            f'ci_lo_{ensemble_type_2}': lo2,\n",
    "            f'ci_hi_{ensemble_type_2}': hi2,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca53b70",
   "metadata": {},
   "source": [
    "### Create the table, view, for the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dce3b8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_report_view_df(df: pd.DataFrame,\n",
    "                          value_vars: list, \n",
    "                          calibration_method: str = 'pure_logits',\n",
    "                          custom_rows: list = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Melt & pivot a stats‐DataFrame so that 'metric'×'dataset' becomes\n",
    "    a two‐level column index, but only for the metrics in value_vars.\n",
    "    \n",
    "    Args:\n",
    "      df         : DataFrame with columns\n",
    "                   ['dataset','ensemble_type','method', *value_vars*]\n",
    "      value_vars : list of the exact column names in `df` to pivot,\n",
    "                   e.g. ['nll_test_mean±CI', 'auc_mean±CI', 'aurac_mean±CI', 'aorac_mean±CI']\n",
    "      custom_rows: list of ensemble_type values in desired order, or None to auto-detect\n",
    "    Returns:\n",
    "      Wide‐form DataFrame indexed by (ensemble_type, method), with\n",
    "      columns MultiIndex (metric_short, dataset_short).\n",
    "    \"\"\"\n",
    "    calibration_methods = df['method'].unique()\n",
    "    if calibration_method not in calibration_methods:\n",
    "        raise ValueError(f\"calibration_method {calibration_method} not in {calibration_methods}\")\n",
    "    #filter based on calibration method\n",
    "    df = df.loc[df['method'] == calibration_method]\n",
    "    # 'nll_test_mean±CI', 'threshold_mean±CI', 'set_size_mean±CI', 'auc_mean±CI', 'aurac_mean±CI', 'aorac_mean±CI'\n",
    "    dfm = df.melt(\n",
    "        id_vars    = ['dataset','ensemble_type','method'],\n",
    "        value_vars = value_vars,\n",
    "        var_name   = 'metric',\n",
    "        value_name = 'value'\n",
    "    )\n",
    "\n",
    "    metric_name_map = {\n",
    "        'nll_test_mean±CI'   : 'NLL',\n",
    "        'threshold_mean±CI'  : 'Threshold',\n",
    "        'set_size_mean±CI'   : 'Set Size',\n",
    "        'auc_mean±CI'        : 'AUC',\n",
    "        'aurac_mean±CI'      : 'AURAC',\n",
    "        'aorac_mean±CI'      : 'AORAC'\n",
    "    }\n",
    "    # 3) map both metric and dataset to their short names\n",
    "    dfm['metric']  = dfm['metric'].map(metric_name_map)\n",
    "    dfm['dataset'] = dfm['dataset'].map({\n",
    "        'SetFit/mnli'                     : 'SetFit',\n",
    "        'ag_news'                         : 'News',\n",
    "        'dbpedia_14'                      : 'DBpedia',\n",
    "        'imdb'                            : 'IMDB',\n",
    "        'mteb/tweet_sentiment_extraction' : 'Tweet',\n",
    "        'stanfordnlp/sst2'                : 'SST-2'\n",
    "    })\n",
    "    \n",
    "    # 4) pivot to a two‐level column index\n",
    "    pivoted = dfm.pivot_table(\n",
    "        index   = ['ensemble_type','method'],\n",
    "        columns = ['metric','dataset'],\n",
    "        values  = 'value',\n",
    "        aggfunc = 'first'\n",
    "    )\n",
    "    \n",
    "    # 5) decide on the exact metric‐order you want (in the same order as value_vars)\n",
    "    metrics_order = [ metric_name_map[v] for v in value_vars ]\n",
    "    datasets_order = ['DBpedia', 'News', 'SST-2', 'SetFit', 'Tweet', 'IMDB']\n",
    "    \n",
    "    # 6) build the MultiIndex of all (metric, dataset) pairs you need\n",
    "    new_cols = pd.MultiIndex.from_product(\n",
    "        [metrics_order, datasets_order],\n",
    "        names=['metric','dataset']\n",
    "    )\n",
    "    \n",
    "    # 7) re‐index the pivoted DataFrame to force that exact column order\n",
    "    wide = pivoted.reindex(columns=new_cols)\n",
    "    \n",
    "    # 8) bring ensemble_type & method back as columns, sort rows, drop any unwanted\n",
    "    wide = wide.reset_index()\n",
    "    \n",
    "    # 9) sort rows by a categorical order of ensemble_type\n",
    "    if custom_rows is None:\n",
    "        # Auto-detect from data if not provided\n",
    "        custom_rows = sorted(df['ensemble_type'].unique())\n",
    "        print(f\"Auto-detected ensemble types: {custom_rows}\")\n",
    "    \n",
    "    wide['ensemble_type'] = pd.Categorical(\n",
    "        wide['ensemble_type'],\n",
    "        categories=custom_rows,\n",
    "        ordered=True\n",
    "    )\n",
    "    wide = wide.sort_values(['ensemble_type','method']).reset_index(drop=True)\n",
    "    \n",
    "    return wide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390cgww8fmb",
   "metadata": {},
   "source": [
    "### LaTeX Table Generation with Statistical Significance Formatting\n",
    "\n",
    "Functions to create publication-ready LaTeX tables with:\n",
    "- **Bold** formatting for best values\n",
    "- **Shading** for values not significantly different from best\n",
    "- Proper LaTeX structure with captions and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2zki22jl9ja",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_mean_ci(value_str: str) -> float:\n",
    "    \"\"\"\n",
    "    Extract mean value from 'mean ± CI' string.\n",
    "    \n",
    "    Args:\n",
    "        value_str: String in format \"0.1234 ± 0.0056\"\n",
    "    \n",
    "    Returns:\n",
    "        float: The mean value\n",
    "    \"\"\"\n",
    "    if pd.isna(value_str) or value_str == 'nan':\n",
    "        return float('nan')\n",
    "    return float(value_str.split(' ± ')[0])\n",
    "\n",
    "def format_cell_value(value_str: str, is_best: bool, is_not_significant: bool) -> str:\n",
    "    \"\"\"\n",
    "    Apply LaTeX formatting to a cell value.\n",
    "    \n",
    "    Args:\n",
    "        value_str: Original value string \"mean ± CI\"\n",
    "        is_best: True if this is the best value in the column\n",
    "        is_not_significant: True if not significantly different from best\n",
    "    \n",
    "    Returns:\n",
    "        str: LaTeX formatted string\n",
    "    \"\"\"\n",
    "    if pd.isna(value_str) or value_str == 'nan':\n",
    "        return ''\n",
    "    \n",
    "    # Start with the original value\n",
    "    formatted = value_str\n",
    "    \n",
    "    # Apply bold if it's the best value\n",
    "    if is_best:\n",
    "        formatted = f\"\\\\textbf{{{formatted}}}\"\n",
    "    \n",
    "    # Apply shading if it's not significantly different from best OR if it is the best\n",
    "    if is_not_significant or is_best:\n",
    "        formatted = f\"\\\\cellcolor{{gray!20}}{formatted}\"\n",
    "    \n",
    "    return formatted\n",
    "\n",
    "def get_significance_for_dataset(df_significance: pd.DataFrame, \n",
    "                                dataset: str, \n",
    "                                ensemble_type: str) -> tuple[bool, bool]:\n",
    "    \"\"\"\n",
    "    Get significance information for a specific dataset-ensemble combination.\n",
    "    \n",
    "    Args:\n",
    "        df_significance: DataFrame from compare_to_best function\n",
    "        dataset: Dataset name (e.g., 'DBpedia', 'News')\n",
    "        ensemble_type: Ensemble type name (ORIGINAL name, not display name)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (is_best, is_not_significant)\n",
    "    \"\"\"\n",
    "    # Map short dataset names back to full names for lookup\n",
    "    dataset_map_reverse = {\n",
    "        'DBpedia': 'dbpedia_14',\n",
    "        'News': 'ag_news', \n",
    "        'SST-2': 'stanfordnlp/sst2',\n",
    "        'SetFit': 'SetFit/mnli',\n",
    "        'Tweet': 'mteb/tweet_sentiment_extraction',\n",
    "        'IMDB': 'imdb'\n",
    "    }\n",
    "    \n",
    "    full_dataset_name = dataset_map_reverse.get(dataset, dataset)\n",
    "    \n",
    "    # Find the row for this dataset and ensemble type\n",
    "    mask = (df_significance['dataset'] == full_dataset_name) & \\\n",
    "           (df_significance['ensemble_type'] == ensemble_type)\n",
    "    \n",
    "    if not mask.any():\n",
    "        return False, False\n",
    "    \n",
    "    row = df_significance[mask].iloc[0]\n",
    "    \n",
    "    # Check if this ensemble type is the best\n",
    "    is_best = (row['ensemble_type'] == row['best_ensemble_type'])\n",
    "    \n",
    "    # Check if it's not significantly different from best\n",
    "    is_not_significant = not row['significant']\n",
    "    \n",
    "    return is_best, is_not_significant\n",
    "\n",
    "def create_latex_table_with_significance(\n",
    "    df_wide: pd.DataFrame,           # Wide-format data from create_report_view_df\n",
    "    df_significance: pd.DataFrame,   # Significance test results\n",
    "    ensemble_type_mapping: dict,     # Original -> Display name mapping\n",
    "    metric_name: str,                # 'NLL', 'AUC', 'AURAC', 'Set Size'\n",
    "    dataset_name: str,               # 'FTC-metadataset', 'Extended', 'Mini'\n",
    "    caption: str,                    # Full caption text\n",
    "    label: str,                      # e.g., 'tab:nll_pure_logits'\n",
    "    maximize: bool = False           # True for AUC/AURAC, False for NLL/Set Size\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Create a publication-ready LaTeX table with statistical significance formatting.\n",
    "    \n",
    "    Args:\n",
    "        df_wide: Wide-format DataFrame with ensemble_type as rows, datasets as columns\n",
    "        df_significance: Results from compare_to_best() function\n",
    "        ensemble_type_mapping: Mapping from original names to display names\n",
    "        metric_name: Name of the metric for the table\n",
    "        dataset_name: Dataset name for caption\n",
    "        caption: Full LaTeX caption\n",
    "        label: LaTeX label for referencing\n",
    "        maximize: True if higher values are better (AUC, AURAC), False otherwise\n",
    "    \n",
    "    Returns:\n",
    "        str: Complete LaTeX table code\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_formatted = df_wide.copy()\n",
    "    \n",
    "    # Handle metric name mapping - 'AUROC' should map to 'AUC' in the data\n",
    "    metric_lookup = {'AUROC': 'AUC', 'Set_Size': 'Set Size'}\n",
    "    data_metric_name = metric_lookup.get(metric_name, metric_name)\n",
    "    \n",
    "    # Get the metric column names (should be tuples like (metric, dataset))\n",
    "    metric_cols = [col for col in df_formatted.columns if isinstance(col, tuple) and col[0] == data_metric_name]\n",
    "    \n",
    "    if not metric_cols:\n",
    "        raise ValueError(f\"No columns found for metric '{data_metric_name}' (original: '{metric_name}') in the DataFrame\")\n",
    "    \n",
    "    # Create reverse mapping from display names to original names for significance lookup\n",
    "    reverse_mapping = {v: k for k, v in ensemble_type_mapping.items()}\n",
    "    \n",
    "    # Process each dataset column\n",
    "    for metric, dataset in metric_cols:\n",
    "        col_name = (metric, dataset)\n",
    "        \n",
    "        # Apply formatting to each cell in this column\n",
    "        for row_idx in range(len(df_formatted)):\n",
    "            # Handle the ensemble_type column access properly with MultiIndex (FIXED)\n",
    "            # Use iloc to access by integer position and then get scalar value\n",
    "            ensemble_display_name = df_formatted.iloc[row_idx]['ensemble_type']\n",
    "            if isinstance(ensemble_display_name, pd.Series):\n",
    "                ensemble_display_name = ensemble_display_name.iloc[0]\n",
    "            \n",
    "            ensemble_original_name = reverse_mapping.get(ensemble_display_name, ensemble_display_name)\n",
    "            \n",
    "            # Get the original value using iloc for MultiIndex (FIXED)\n",
    "            original_value = df_formatted.iloc[row_idx][col_name]\n",
    "            if isinstance(original_value, pd.Series):\n",
    "                original_value = original_value.iloc[0]\n",
    "            \n",
    "            if pd.isna(original_value) or str(original_value) == 'nan':\n",
    "                # Set empty string using loc for MultiIndex\n",
    "                df_formatted.loc[row_idx, col_name] = ''\n",
    "                continue\n",
    "            \n",
    "            # Get significance information  \n",
    "            is_best, is_not_significant = get_significance_for_dataset(\n",
    "                df_significance, dataset, ensemble_original_name\n",
    "            )\n",
    "            \n",
    "            # Apply formatting\n",
    "            formatted_value = format_cell_value(str(original_value), is_best, is_not_significant)\n",
    "            # Set value using loc for MultiIndex\n",
    "            df_formatted.loc[row_idx, col_name] = formatted_value\n",
    "    \n",
    "    # Build the LaTeX table structure (also fixing data access here)\n",
    "    datasets = [col[1] for col in metric_cols]  # Extract dataset names\n",
    "    n_datasets = len(datasets)\n",
    "    \n",
    "    # Column specification: l for ensemble type + c for each dataset\n",
    "    col_spec = 'l' + 'c' * n_datasets\n",
    "    \n",
    "    # Build header row - FIXED: single backslashes\n",
    "    header = ' & '.join(['Ensemble Type'] + datasets) + ' \\\\\\\\'\n",
    "    \n",
    "    # Build data rows (FIXED for MultiIndex)\n",
    "    data_rows = []\n",
    "    for row_idx in range(len(df_formatted)):\n",
    "        ensemble_name = df_formatted.iloc[row_idx]['ensemble_type']\n",
    "        if isinstance(ensemble_name, pd.Series):\n",
    "            ensemble_name = ensemble_name.iloc[0]\n",
    "        \n",
    "        values = []\n",
    "        for col in metric_cols:\n",
    "            value = df_formatted.iloc[row_idx][col]\n",
    "            if isinstance(value, pd.Series):\n",
    "                value = value.iloc[0]\n",
    "            values.append(str(value))\n",
    "        \n",
    "        row_str = ' & '.join([str(ensemble_name)] + values) + ' \\\\\\\\'\n",
    "        data_rows.append(row_str)\n",
    "    \n",
    "    # Create separators for different ensemble groups if needed\n",
    "    formatted_rows = []\n",
    "    for i, row_str in enumerate(data_rows):\n",
    "        if i > 0 and 'Greedy-50' in row_str:\n",
    "            # Add midrule before first Greedy-50 row\n",
    "            prev_row = data_rows[i-1]\n",
    "            if 'Greedy-5' in prev_row and 'Greedy-50' not in prev_row:\n",
    "                formatted_rows.append('\\\\midrule')\n",
    "        formatted_rows.append(row_str)\n",
    "    \n",
    "    # Combine everything into the full LaTeX table - FIXED: single backslashes\n",
    "    latex_lines = [\n",
    "        '\\\\begin{table}[h]',\n",
    "        f'  \\\\caption{{{caption}}}',\n",
    "        f'  \\\\label{{{label}}}',\n",
    "        '  \\\\centering',\n",
    "        '  \\\\resizebox{\\\\textwidth}{!}{%',\n",
    "        f'  \\\\begin{{tabular}}{{{col_spec}}}',\n",
    "        '    \\\\toprule',\n",
    "        f'    {header}',\n",
    "        '    \\\\midrule'\n",
    "    ]\n",
    "    \n",
    "    # Add data rows\n",
    "    for row in formatted_rows:\n",
    "        latex_lines.append(f'    {row}')\n",
    "    \n",
    "    # Close the table\n",
    "    latex_lines.extend([\n",
    "        '    \\\\bottomrule',\n",
    "        '  \\\\end{tabular}',\n",
    "        '  }',\n",
    "        '\\\\end{table}'\n",
    "    ])\n",
    "    \n",
    "    # Return with proper newlines\n",
    "    return '\\n'.join(latex_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af85de2c",
   "metadata": {},
   "source": [
    "## Load Pre-computed Metrics\n",
    "\n",
    "Load metrics that have already been computed in iclr_llm_analysis_plots_v2.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7760bb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded latest metrics from plots_v2 (2025-09-21)\n",
      "Extended 99%: (240, 18)\n",
      "Mini 99%: (240, 18)\n",
      "Columns available: ['dataset', 'seed', 'method', 'ensemble_type', 'ensemble_size', 'ensemble_unique_size', 'nll_test', 'c1', 'c2', 'epi_scalar', 'ensemble_time', 'calibration_time', 'path', 'threshold', 'set_size', 'auc', 'aurac', 'aorac']\n"
     ]
    }
   ],
   "source": [
    "# Load pre-computed metrics from iclr_llm_analysis_plots_v2.ipynb\n",
    "# These files contain all computed metrics (NLL, AUC, AURAC, set_size, etc.)\n",
    "\n",
    "# Load metrics files computed with different dates\n",
    "try:\n",
    "    # Try latest files first (from plots_v2 notebook)\n",
    "    df_extended_99 = read_file('metrics/extended_with_metrics_cov_0p99_2025-09-21.csv')\n",
    "    df_extended_999 = read_file('metrics/extended_with_metrics_cov_0p999_2025-09-21.csv')\n",
    "    df_mini_99 = read_file('metrics/mini_with_metrics_cov_0p99_2025-09-21.csv')\n",
    "    df_mini_999 = read_file('metrics/mini_with_metrics_cov_0p999_2025-09-21.csv')\n",
    "    \n",
    "    print(\"✓ Loaded latest metrics from plots_v2 (2025-09-21)\")\n",
    "    print(f\"Extended 99%: {df_extended_99.shape}\")\n",
    "    print(f\"Mini 99%: {df_mini_99.shape}\")\n",
    "    print(f\"Columns available: {df_extended_99.columns.tolist()}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Latest metrics files not found, trying fallback files...\")\n",
    "    # Try older files as fallback  \n",
    "    try:\n",
    "        df_extended_99 = read_file('metrics/ftc_with_metrics_cov_0p99_09_10.csv')\n",
    "        df_extended_999 = read_file('metrics/ftc_with_metrics_cov_0p999_09_10.csv')\n",
    "        df_mini_1_99 = read_file('metrics/mini_1_with_metrics_cov_0p99_09_10.csv')\n",
    "        df_mini_2_99 = read_file('metrics/mini_2_with_metrics_cov_0p99_09_10.csv')\n",
    "        df_mini_1_999 = read_file('metrics/mini_1_with_metrics_cov_0p999_09_10.csv')\n",
    "        df_mini_2_999 = read_file('metrics/mini_2_with_metrics_cov_0p999_09_10.csv')\n",
    "        \n",
    "        # Combine mini datasets\n",
    "        df_mini_99 = combine_and_clean_dataframes(df_mini_1_99, df_mini_2_99)\n",
    "        df_mini_999 = combine_and_clean_dataframes(df_mini_1_999, df_mini_2_999)\n",
    "        \n",
    "        print(\"✓ Loaded fallback metrics (09_10)\")\n",
    "        print(f\"Extended 99%: {df_extended_99.shape}\")\n",
    "        print(f\"Mini 99%: {df_mini_99.shape}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\" No pre-computed metrics found!\")\n",
    "        print(\"Please run iclr_llm_analysis_plots_v2.ipynb first to compute metrics.\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5w4fq6ka0lx",
   "metadata": {},
   "source": [
    "## Generate LaTeX Tables from Pre-computed Metrics\n",
    "\n",
    "Create tables for all available metrics using the loaded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "pvb10rzlgk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data for pure_logits method:\n",
      "Extended 99%: (240, 18)\n",
      "Mini 99%: (240, 18)\n",
      "Available ensemble types: ['greedy_50_baseline', 'greedy_50_post_calib', 'greedy_50_temp_calibrate_then_pool', 'greedy_50_temp_pool_then_calibrate', 'greedy_5_baseline', 'greedy_5_post_calib', 'greedy_5_temp_calibrate_then_pool', 'greedy_5_temp_pool_then_calibrate']\n",
      "Available metrics columns:\n",
      "['nll_test', 'threshold', 'set_size', 'auc', 'aurac']\n",
      "Table columns to include: ['dataset', 'method', 'ensemble_type', 'nll_test_mean±CI', 'auc_mean±CI', 'set_size_mean±CI', 'aurac_mean±CI']\n",
      "✓ Processed metrics for table generation\n"
     ]
    }
   ],
   "source": [
    "# Process loaded metrics for table generation\n",
    "method_of_interest = 'pure_logits'\n",
    "\n",
    "# Filter for pure_logits method only\n",
    "df_extended_99_filtered = df_extended_99.loc[df_extended_99['method'] == method_of_interest]\n",
    "df_extended_999_filtered = df_extended_999.loc[df_extended_999['method'] == method_of_interest]\n",
    "df_mini_99_filtered = df_mini_99.loc[df_mini_99['method'] == method_of_interest]\n",
    "df_mini_999_filtered = df_mini_999.loc[df_mini_999['method'] == method_of_interest]\n",
    "\n",
    "print(\"Filtered data for pure_logits method:\")\n",
    "print(f\"Extended 99%: {df_extended_99_filtered.shape}\")\n",
    "print(f\"Mini 99%: {df_mini_99_filtered.shape}\")\n",
    "print(f\"Available ensemble types: {sorted(df_extended_99_filtered['ensemble_type'].unique())}\")\n",
    "print(\"Available metrics columns:\")\n",
    "metric_cols = [col for col in df_extended_99_filtered.columns if col in ['nll_test', 'auc', 'aurac', 'set_size', 'threshold']]\n",
    "print(metric_cols)\n",
    "\n",
    "# Calculate confidence intervals for all datasets\n",
    "df_extended_99_ci = calc_ci_for_df(df_extended_99_filtered.copy())\n",
    "df_extended_999_ci = calc_ci_for_df(df_extended_999_filtered.copy())\n",
    "df_mini_99_ci = calc_ci_for_df(df_mini_99_filtered.copy())\n",
    "df_mini_999_ci = calc_ci_for_df(df_mini_999_filtered.copy())\n",
    "\n",
    "# Create subset dataframes with the metrics we need for tables\n",
    "subset_cols = ['dataset', 'method', 'ensemble_type', 'nll_test_mean±CI', 'auc_mean±CI', 'set_size_mean±CI']\n",
    "if 'aurac' in df_extended_99_filtered.columns:\n",
    "    subset_cols.append('aurac_mean±CI')\n",
    "\n",
    "print(f\"Table columns to include: {subset_cols}\")\n",
    "\n",
    "df_extended_99_subset = flatten_subset_df(df_extended_99_ci, subset=subset_cols)\n",
    "df_extended_999_subset = flatten_subset_df(df_extended_999_ci, subset=subset_cols)\n",
    "df_mini_99_subset = flatten_subset_df(df_mini_99_ci, subset=subset_cols)\n",
    "df_mini_999_subset = flatten_subset_df(df_mini_999_ci, subset=subset_cols)\n",
    "\n",
    "print(\"✓ Processed metrics for table generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ga2tbm1a6bi",
   "metadata": {},
   "source": [
    "## Discover Available Ensemble Types\n",
    "\n",
    "Check what ensemble types are actually in the loaded data to create proper mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ias17zhbuy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ENSEMBLE TYPES IN LOADED DATA ===\n",
      "Use this information to update the mapping dictionaries below!\n",
      "\n",
      " Extended dataset ensemble types:\n",
      "  1. 'greedy_50_baseline'\n",
      "  2. 'greedy_50_post_calib'\n",
      "  3. 'greedy_50_temp_calibrate_then_pool'\n",
      "  4. 'greedy_50_temp_pool_then_calibrate'\n",
      "  5. 'greedy_5_baseline'\n",
      "  6. 'greedy_5_post_calib'\n",
      "  7. 'greedy_5_temp_calibrate_then_pool'\n",
      "  8. 'greedy_5_temp_pool_then_calibrate'\n",
      "\n",
      "Mini dataset ensemble types:\n",
      "  1. 'greedy_50_baseline'\n",
      "  2. 'greedy_50_post_calib'\n",
      "  3. 'greedy_50_temp_calibrate_then_pool'\n",
      "  4. 'greedy_50_temp_pool_then_calibrate'\n",
      "  5. 'greedy_5_baseline'\n",
      "  6. 'greedy_5_post_calib'\n",
      "  7. 'greedy_5_temp_calibrate_then_pool'\n",
      "  8. 'greedy_5_temp_pool_then_calibrate'\n",
      "\n",
      "============================================================\n",
      "COPY THE TYPES ABOVE TO UPDATE THE MAPPINGS BELOW\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Discover what ensemble types are in the loaded data\n",
    "print(\"=== ENSEMBLE TYPES IN LOADED DATA ===\")\n",
    "print(\"Use this information to update the mapping dictionaries below!\\n\")\n",
    "\n",
    "print(\" Extended dataset ensemble types:\")\n",
    "for i, et in enumerate(sorted(df_extended_99_filtered['ensemble_type'].unique()), 1):\n",
    "    print(f\"  {i}. '{et}'\")\n",
    "\n",
    "print(\"\\nMini dataset ensemble types:\")\n",
    "for i, et in enumerate(sorted(df_mini_99_filtered['ensemble_type'].unique()), 1):\n",
    "    print(f\"  {i}. '{et}'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COPY THE TYPES ABOVE TO UPDATE THE MAPPINGS BELOW\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xj0extvxwf",
   "metadata": {},
   "source": [
    "## Define Ensemble Type Mappings\n",
    "\n",
    "Update these mappings based on the output above to get proper table labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5yvtnyanjj4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Mapping dictionaries and ordering patterns defined\n",
      "✓ Extended mapping has 8 entries\n",
      "✓ FTC mapping has 8 entries\n",
      "✓ Extended order: ['Greedy-5', 'Greedy-5 p.t.c.', 'Greedy-5 c.t.p.', 'Greedy-5 JUCAL', 'Greedy-50', 'Greedy-50 p.t.c.', 'Greedy-50 c.t.p.', 'Greedy-50 JUCAL']\n",
      "✓ FTC order: ['G5', 'G5 p.t.c.', 'G5 JUCAL', 'G50', 'G50 p.t.c.', 'G50 JUCAL', 'G50 r.c.o. JUCAL', 'G50 r.c. JUCAL']\n"
     ]
    }
   ],
   "source": [
    "# Mapping for Extended/Mini datasets (from plots_v2)\n",
    "etype_map_extended = {\n",
    "    'greedy_5_baseline': 'Greedy-5',\n",
    "    'greedy_5_temp_pool_then_calibrate': 'Greedy-5 p.t.c.',\n",
    "    'greedy_5_temp_calibrate_then_pool': 'Greedy-5 c.t.p.',\n",
    "    'greedy_5_post_calib': 'Greedy-5 JUCAL',\n",
    "    'greedy_50_baseline': 'Greedy-50',\n",
    "    'greedy_50_temp_pool_then_calibrate': 'Greedy-50 p.t.c.',\n",
    "    'greedy_50_temp_calibrate_then_pool': 'Greedy-50 c.t.p.',\n",
    "    'greedy_50_post_calib': 'Greedy-50 JUCAL'\n",
    "}\n",
    "\n",
    "# Mapping for FTC dataset (April 2025 data)\n",
    "etype_map_ftc = {\n",
    "    'greedy_unique_5_baseline': 'G5',\n",
    "    'greedy_unique_5_temp_baseline': 'G5 p.t.c.',\n",
    "    'greedy_unique_5_post_calib': 'G5 JUCAL',\n",
    "    'greedy_50_baseline': 'G50',\n",
    "    'greedy_50_temp_baseline': 'G50 p.t.c.',\n",
    "    'greedy_50_post_calib': 'G50 JUCAL',\n",
    "    'greedy_50_calib_once': 'G50 r.c.o. JUCAL',\n",
    "    'greedy_50_calib_every_step': 'G50 r.c. JUCAL'\n",
    "}\n",
    "\n",
    "# Define the correct ordering patterns for each dataset type\n",
    "ensemble_order_extended = [\n",
    "    'Greedy-5',\n",
    "    'Greedy-5 p.t.c.',\n",
    "    'Greedy-5 c.t.p.',\n",
    "    'Greedy-5 JUCAL',\n",
    "    'Greedy-50',\n",
    "    'Greedy-50 p.t.c.',\n",
    "    'Greedy-50 c.t.p.',\n",
    "    'Greedy-50 JUCAL'\n",
    "]\n",
    "\n",
    "ensemble_order_ftc = [\n",
    "    'G5',\n",
    "    'G5 p.t.c.',\n",
    "    'G5 JUCAL',\n",
    "    'G50',\n",
    "    'G50 p.t.c.',\n",
    "    'G50 JUCAL',\n",
    "    'G50 r.c.o. JUCAL',\n",
    "    'G50 r.c. JUCAL'\n",
    "]\n",
    "\n",
    "def get_ensemble_order_for_dataset(available_types, dataset_type='extended'):\n",
    "    \"\"\"\n",
    "    Get ensemble types in the correct logical order based on dataset type.\n",
    "    \n",
    "    Args:\n",
    "        available_types: List of ensemble types available in the data\n",
    "        dataset_type: 'extended', 'mini', or 'ftc'\n",
    "    \n",
    "    Returns:\n",
    "        List of ensemble types in correct order\n",
    "    \"\"\"\n",
    "    if dataset_type in ['extended', 'mini']:\n",
    "        preferred_order = ensemble_order_extended\n",
    "    elif dataset_type == 'ftc':\n",
    "        preferred_order = ensemble_order_ftc\n",
    "    else:\n",
    "        # Fallback to alphabetical if unknown type\n",
    "        return sorted(available_types)\n",
    "    \n",
    "    # Filter to only include available types in the correct order\n",
    "    ordered_types = [t for t in preferred_order if t in available_types]\n",
    "    \n",
    "    # Add any remaining types that don't match patterns (fallback)\n",
    "    for t in available_types:\n",
    "        if t not in ordered_types:\n",
    "            ordered_types.append(t)\n",
    "            \n",
    "    return ordered_types\n",
    "\n",
    "print(\"✓ Mapping dictionaries and ordering patterns defined\")\n",
    "print(f\"✓ Extended mapping has {len(etype_map_extended)} entries\")\n",
    "print(f\"✓ FTC mapping has {len(etype_map_ftc)} entries\")\n",
    "print(f\"✓ Extended order: {ensemble_order_extended}\")\n",
    "print(f\"✓ FTC order: {ensemble_order_ftc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff105c7",
   "metadata": {},
   "source": [
    "### Running significance tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "h8tsdlhfxq9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running significance tests...\n",
      "=== Extended Dataset Significance Tests ===\n",
      "Using z=1.96 for alpha=0.05\n",
      "Auto-detected ensemble types: ['greedy_50_baseline', 'greedy_50_post_calib', 'greedy_50_temp_calibrate_then_pool', 'greedy_50_temp_pool_then_calibrate', 'greedy_5_baseline', 'greedy_5_post_calib', 'greedy_5_temp_calibrate_then_pool', 'greedy_5_temp_pool_then_calibrate']\n",
      "greedy_5_post_calib is the best ensemble type for SetFit/mnli with aurac=0.9240 \n",
      "greedy_50_post_calib is the best ensemble type for ag_news with aurac=0.9835 \n",
      "greedy_50_post_calib is the best ensemble type for dbpedia_14 with aurac=0.9897 \n",
      "greedy_5_baseline is the best ensemble type for imdb with aurac=0.9859 \n",
      "greedy_50_temp_calibrate_then_pool is the best ensemble type for mteb/tweet_sentiment_extraction with aurac=0.9246 \n",
      "greedy_50_temp_calibrate_then_pool is the best ensemble type for stanfordnlp/sst2 with aurac=0.9849 \n",
      "✓ Extended AURAC significance: (48, 11)\n",
      "Using z=1.96 for alpha=0.05\n",
      "Auto-detected ensemble types: ['greedy_50_baseline', 'greedy_50_post_calib', 'greedy_50_temp_calibrate_then_pool', 'greedy_50_temp_pool_then_calibrate', 'greedy_5_baseline', 'greedy_5_post_calib', 'greedy_5_temp_calibrate_then_pool', 'greedy_5_temp_pool_then_calibrate']\n",
      "greedy_5_post_calib is the best ensemble type for SetFit/mnli with nll_test=0.4965 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greedy_50_post_calib is the best ensemble type for ag_news with nll_test=0.1423 \n",
      "greedy_50_post_calib is the best ensemble type for dbpedia_14 with nll_test=0.0288 \n",
      "greedy_50_post_calib is the best ensemble type for imdb with nll_test=0.0983 \n",
      "greedy_50_post_calib is the best ensemble type for mteb/tweet_sentiment_extraction with nll_test=0.4680 \n",
      "greedy_50_post_calib is the best ensemble type for stanfordnlp/sst2 with nll_test=0.1090 \n",
      "Using z=1.96 for alpha=0.05\n",
      "Auto-detected ensemble types: ['greedy_50_baseline', 'greedy_50_post_calib', 'greedy_50_temp_calibrate_then_pool', 'greedy_50_temp_pool_then_calibrate', 'greedy_5_baseline', 'greedy_5_post_calib', 'greedy_5_temp_calibrate_then_pool', 'greedy_5_temp_pool_then_calibrate']\n",
      "greedy_5_post_calib is the best ensemble type for SetFit/mnli with auc=0.9377 \n",
      "greedy_50_post_calib is the best ensemble type for ag_news with auc=0.9948 \n",
      "greedy_50_post_calib is the best ensemble type for dbpedia_14 with auc=0.9999 \n",
      "greedy_5_temp_calibrate_then_pool is the best ensemble type for imdb with auc=0.9935 \n",
      "greedy_50_temp_calibrate_then_pool is the best ensemble type for mteb/tweet_sentiment_extraction with auc=0.9411 \n",
      "greedy_50_temp_calibrate_then_pool is the best ensemble type for stanfordnlp/sst2 with auc=0.9918 \n",
      "Using z=1.96 for alpha=0.05\n",
      "Auto-detected ensemble types: ['greedy_50_baseline', 'greedy_50_post_calib', 'greedy_50_temp_calibrate_then_pool', 'greedy_50_temp_pool_then_calibrate', 'greedy_5_baseline', 'greedy_5_post_calib', 'greedy_5_temp_calibrate_then_pool', 'greedy_5_temp_pool_then_calibrate']\n",
      "greedy_50_post_calib is the best ensemble type for SetFit/mnli with set_size=2.2334 \n",
      "greedy_50_post_calib is the best ensemble type for ag_news with set_size=1.2228 \n",
      "greedy_50_baseline is the best ensemble type for dbpedia_14 with set_size=1.0000 \n",
      "greedy_50_temp_calibrate_then_pool is the best ensemble type for imdb with set_size=1.0992 \n",
      "greedy_50_post_calib is the best ensemble type for mteb/tweet_sentiment_extraction with set_size=2.0633 \n",
      "greedy_50_post_calib is the best ensemble type for stanfordnlp/sst2 with set_size=1.1385 \n",
      "✓ Extended NLL significance: (48, 11)\n",
      "✓ Extended AUC significance: (48, 11)\n",
      "✓ Extended Set Size significance: (48, 11)\n",
      "\n",
      "=== Mini Dataset Significance Tests ===\n",
      "Using z=1.96 for alpha=0.05\n",
      "Auto-detected ensemble types: ['greedy_50_baseline', 'greedy_50_post_calib', 'greedy_50_temp_calibrate_then_pool', 'greedy_50_temp_pool_then_calibrate', 'greedy_5_baseline', 'greedy_5_post_calib', 'greedy_5_temp_calibrate_then_pool', 'greedy_5_temp_pool_then_calibrate']\n",
      "greedy_50_temp_calibrate_then_pool is the best ensemble type for SetFit/mnli with aurac=0.9556 \n",
      "greedy_50_temp_calibrate_then_pool is the best ensemble type for ag_news with aurac=0.9783 \n",
      "greedy_50_post_calib is the best ensemble type for dbpedia_14 with aurac=0.9896 \n",
      "greedy_50_temp_calibrate_then_pool is the best ensemble type for imdb with aurac=0.9832 \n",
      "greedy_50_temp_calibrate_then_pool is the best ensemble type for mteb/tweet_sentiment_extraction with aurac=0.9113 \n",
      "greedy_50_temp_calibrate_then_pool is the best ensemble type for stanfordnlp/sst2 with aurac=0.9832 \n",
      "✓ Mini AURAC significance: (48, 11)\n",
      "Using z=1.96 for alpha=0.05\n",
      "Auto-detected ensemble types: ['greedy_50_baseline', 'greedy_50_post_calib', 'greedy_50_temp_calibrate_then_pool', 'greedy_50_temp_pool_then_calibrate', 'greedy_5_baseline', 'greedy_5_post_calib', 'greedy_5_temp_calibrate_then_pool', 'greedy_5_temp_pool_then_calibrate']\n",
      "greedy_50_post_calib is the best ensemble type for SetFit/mnli with nll_test=0.3480 \n",
      "greedy_50_post_calib is the best ensemble type for ag_news with nll_test=0.1899 \n",
      "greedy_50_post_calib is the best ensemble type for dbpedia_14 with nll_test=0.0306 \n",
      "greedy_50_temp_pool_then_calibrate is the best ensemble type for imdb with nll_test=0.1255 \n",
      "greedy_50_post_calib is the best ensemble type for mteb/tweet_sentiment_extraction with nll_test=0.4979 \n",
      "greedy_50_post_calib is the best ensemble type for stanfordnlp/sst2 with nll_test=0.1309 \n",
      "Using z=1.96 for alpha=0.05\n",
      "Auto-detected ensemble types: ['greedy_50_baseline', 'greedy_50_post_calib', 'greedy_50_temp_calibrate_then_pool', 'greedy_50_temp_pool_then_calibrate', 'greedy_5_baseline', 'greedy_5_post_calib', 'greedy_5_temp_calibrate_then_pool', 'greedy_5_temp_pool_then_calibrate']\n",
      "greedy_50_temp_calibrate_then_pool is the best ensemble type for SetFit/mnli with auc=0.9671 \n",
      "greedy_50_temp_calibrate_then_pool is the best ensemble type for ag_news with auc=0.9909 \n",
      "greedy_50_post_calib is the best ensemble type for dbpedia_14 with auc=0.9998 \n",
      "greedy_50_temp_calibrate_then_pool is the best ensemble type for imdb with auc=0.9897 \n",
      "greedy_50_temp_calibrate_then_pool is the best ensemble type for mteb/tweet_sentiment_extraction with auc=0.9316 \n",
      "greedy_50_temp_calibrate_then_pool is the best ensemble type for stanfordnlp/sst2 with auc=0.9890 \n",
      "Using z=1.96 for alpha=0.05\n",
      "Auto-detected ensemble types: ['greedy_50_baseline', 'greedy_50_post_calib', 'greedy_50_temp_calibrate_then_pool', 'greedy_50_temp_pool_then_calibrate', 'greedy_5_baseline', 'greedy_5_post_calib', 'greedy_5_temp_calibrate_then_pool', 'greedy_5_temp_pool_then_calibrate']\n",
      "greedy_50_post_calib is the best ensemble type for SetFit/mnli with set_size=1.8980 \n",
      "greedy_5_post_calib is the best ensemble type for ag_news with set_size=1.4131 \n",
      "greedy_50_baseline is the best ensemble type for dbpedia_14 with set_size=1.0000 \n",
      "greedy_50_temp_calibrate_then_pool is the best ensemble type for imdb with set_size=1.1660 \n",
      "greedy_50_post_calib is the best ensemble type for mteb/tweet_sentiment_extraction with set_size=2.1470 \n",
      "greedy_50_temp_calibrate_then_pool is the best ensemble type for stanfordnlp/sst2 with set_size=1.1862 \n",
      "✓ Mini NLL significance: (48, 11)\n",
      "✓ Mini AUC significance: (48, 11)\n",
      "✓ Mini Set Size significance: (48, 11)\n",
      "\n",
      "✓ All significance tests completed\n"
     ]
    }
   ],
   "source": [
    "# Generate significance tests for all metrics\n",
    "print(\"Running significance tests...\")\n",
    "\n",
    "# Extended dataset significance tests\n",
    "print(\"=== Extended Dataset Significance Tests ===\")\n",
    "if 'aurac' in df_extended_99_filtered.columns:\n",
    "    df_extended_sig_aurac_99 = compare_to_best(df_extended_99_ci, metric='aurac', calibration_method='pure_logits', alpha=0.05)\n",
    "    print(f\"✓ Extended AURAC significance: {df_extended_sig_aurac_99.shape}\")\n",
    "\n",
    "df_extended_sig_nll_99 = compare_to_best(df_extended_99_ci, metric='nll_test', calibration_method='pure_logits', alpha=0.05)\n",
    "df_extended_sig_auc_99 = compare_to_best(df_extended_99_ci, metric='auc', calibration_method='pure_logits', alpha=0.05)\n",
    "df_extended_sig_setsize_99 = compare_to_best(df_extended_99_ci, metric='set_size', calibration_method='pure_logits', alpha=0.05)\n",
    "\n",
    "print(f\"✓ Extended NLL significance: {df_extended_sig_nll_99.shape}\")\n",
    "print(f\"✓ Extended AUC significance: {df_extended_sig_auc_99.shape}\")\n",
    "print(f\"✓ Extended Set Size significance: {df_extended_sig_setsize_99.shape}\")\n",
    "\n",
    "# Mini dataset significance tests\n",
    "print(\"\\n=== Mini Dataset Significance Tests ===\")\n",
    "if 'aurac' in df_mini_99_filtered.columns:\n",
    "    df_mini_sig_aurac_99 = compare_to_best(df_mini_99_ci, metric='aurac', calibration_method='pure_logits', alpha=0.05)\n",
    "    print(f\"✓ Mini AURAC significance: {df_mini_sig_aurac_99.shape}\")\n",
    "\n",
    "df_mini_sig_nll_99 = compare_to_best(df_mini_99_ci, metric='nll_test', calibration_method='pure_logits', alpha=0.05)\n",
    "df_mini_sig_auc_99 = compare_to_best(df_mini_99_ci, metric='auc', calibration_method='pure_logits', alpha=0.05)\n",
    "df_mini_sig_setsize_99 = compare_to_best(df_mini_99_ci, metric='set_size', calibration_method='pure_logits', alpha=0.05)\n",
    "\n",
    "print(f\"✓ Mini NLL significance: {df_mini_sig_nll_99.shape}\")\n",
    "print(f\"✓ Mini AUC significance: {df_mini_sig_auc_99.shape}\")\n",
    "print(f\"✓ Mini Set Size significance: {df_mini_sig_setsize_99.shape}\")\n",
    "\n",
    "print(\"\\n✓ All significance tests completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a4d5f5",
   "metadata": {},
   "source": [
    "### Generate LaTeX tables for all metrics (NLL, AUC, AURAC, Set Size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "mza1s1b14l",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating LaTeX tables...\n",
      "Extended ensemble types (proper order): ['Greedy-5', 'Greedy-5 p.t.c.', 'Greedy-5 c.t.p.', 'Greedy-5 JUCAL', 'Greedy-50', 'Greedy-50 p.t.c.', 'Greedy-50 c.t.p.', 'Greedy-50 JUCAL']\n",
      "Mini ensemble types (proper order): ['Greedy-5', 'Greedy-5 p.t.c.', 'Greedy-5 c.t.p.', 'Greedy-5 JUCAL', 'Greedy-50', 'Greedy-50 p.t.c.', 'Greedy-50 c.t.p.', 'Greedy-50 JUCAL']\n",
      "\n",
      "=== Extended Dataset Tables ===\n",
      "\n",
      " EXTENDED AURAC TABLE:\n",
      "\\begin{tabular}{llllllll}\n",
      "\\toprule\n",
      "ensemble_type & method & \\multicolumn{6}{r}{AURAC} \\\\\n",
      " &  & DBpedia & News & SST-2 & SetFit & Tweet & IMDB \\\\\n",
      "\\midrule\n",
      "Greedy-5 & pure_logits & 0.9895 ± 0.0 & 0.981 ± 0.0011 & 0.984 ± 0.0005 & 0.8915 ± 0.0008 & 0.9103 ± 0.0028 & 0.9859 ± 0.0002 \\\\\n",
      "Greedy-5 p.t.c. & pure_logits & 0.9895 ± 0.0 & 0.981 ± 0.0011 & 0.984 ± 0.0005 & 0.8915 ± 0.0008 & 0.9103 ± 0.0027 & 0.9859 ± 0.0002 \\\\\n",
      "Greedy-5 c.t.p. & pure_logits & 0.9896 ± 0.0 & 0.9819 ± 0.0008 & 0.9842 ± 0.0005 & 0.9186 ± 0.0007 & 0.9187 ± 0.0011 & 0.9859 ± 0.0003 \\\\\n",
      "Greedy-5 JUCAL & pure_logits & 0.9897 ± 0.0 & 0.9829 ± 0.0005 & 0.9842 ± 0.0005 & 0.924 ± 0.0006 & 0.9211 ± 0.0006 & 0.9858 ± 0.0002 \\\\\n",
      "Greedy-50 & pure_logits & 0.9895 ± 0.0 & 0.981 ± 0.0008 & 0.9833 ± 0.0005 & 0.9023 ± 0.0006 & 0.9157 ± 0.0021 & 0.9838 ± 0.0003 \\\\\n",
      "Greedy-50 p.t.c. & pure_logits & 0.9895 ± 0.0 & 0.981 ± 0.0008 & 0.9833 ± 0.0005 & 0.9023 ± 0.0006 & 0.9158 ± 0.0021 & 0.9838 ± 0.0003 \\\\\n",
      "Greedy-50 c.t.p. & pure_logits & 0.9896 ± 0.0 & 0.9831 ± 0.0006 & 0.9849 ± 0.0004 & 0.9215 ± 0.0006 & 0.9246 ± 0.0014 & 0.9856 ± 0.0002 \\\\\n",
      "Greedy-50 JUCAL & pure_logits & 0.9897 ± 0.0 & 0.9835 ± 0.0005 & 0.9849 ± 0.0005 & 0.9237 ± 0.0007 & 0.9236 ± 0.0014 & 0.9855 ± 0.0002 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "✓ Extended AURAC table: (8, 8)\n",
      "\n",
      "EXTENDED NLL TABLE:\n",
      "\\begin{tabular}{llllllll}\n",
      "\\toprule\n",
      "ensemble_type & method & \\multicolumn{6}{r}{NLL} \\\\\n",
      " &  & DBpedia & News & SST-2 & SetFit & Tweet & IMDB \\\\\n",
      "\\midrule\n",
      "Greedy-5 & pure_logits & 0.0376 ± 0.0005 & 0.1682 ± 0.0048 & 0.1359 ± 0.0051 & 0.5465 ± 0.0033 & 0.5095 ± 0.0089 & 0.1171 ± 0.0028 \\\\\n",
      "Greedy-5 p.t.c. & pure_logits & 0.0348 ± 0.0007 & 0.1618 ± 0.0052 & 0.1208 ± 0.004 & 0.5431 ± 0.0019 & 0.5012 ± 0.0052 & 0.1018 ± 0.0022 \\\\\n",
      "Greedy-5 c.t.p. & pure_logits & 0.0324 ± 0.0006 & 0.1613 ± 0.0043 & 0.1235 ± 0.0035 & 0.54 ± 0.001 & 0.5239 ± 0.0152 & 0.104 ± 0.0021 \\\\\n",
      "Greedy-5 JUCAL & pure_logits & 0.029 ± 0.0004 & 0.1479 ± 0.0023 & 0.1143 ± 0.0032 & 0.4965 ± 0.0013 & 0.4772 ± 0.0028 & 0.1005 ± 0.0018 \\\\\n",
      "Greedy-50 & pure_logits & 0.0349 ± 0.0005 & 0.1541 ± 0.0043 & 0.1137 ± 0.0039 & 0.531 ± 0.0016 & 0.4763 ± 0.0052 & 0.105 ± 0.0026 \\\\\n",
      "Greedy-50 p.t.c. & pure_logits & 0.0331 ± 0.0003 & 0.151 ± 0.0037 & 0.113 ± 0.0035 & 0.5309 ± 0.0016 & 0.4758 ± 0.0049 & 0.1042 ± 0.0019 \\\\\n",
      "Greedy-50 c.t.p. & pure_logits & 0.0308 ± 0.0003 & 0.1512 ± 0.002 & 0.122 ± 0.001 & 0.5439 ± 0.0016 & 0.5112 ± 0.0027 & 0.109 ± 0.0014 \\\\\n",
      "Greedy-50 JUCAL & pure_logits & 0.0288 ± 0.0004 & 0.1423 ± 0.0024 & 0.109 ± 0.0032 & 0.4972 ± 0.0018 & 0.468 ± 0.0045 & 0.0983 ± 0.0017 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\n",
      "EXTENDED AUC TABLE:\n",
      "\\begin{tabular}{llllllll}\n",
      "\\toprule\n",
      "ensemble_type & method & \\multicolumn{6}{r}{AUC} \\\\\n",
      " &  & DBpedia & News & SST-2 & SetFit & Tweet & IMDB \\\\\n",
      "\\midrule\n",
      "Greedy-5 & pure_logits & 0.9998 ± 0.0 & 0.9929 ± 0.0007 & 0.9907 ± 0.0007 & 0.9144 ± 0.0008 & 0.9316 ± 0.0019 & 0.9934 ± 0.0003 \\\\\n",
      "Greedy-5 p.t.c. & pure_logits & 0.9998 ± 0.0 & 0.9929 ± 0.0007 & 0.9907 ± 0.0007 & 0.9144 ± 0.0008 & 0.9316 ± 0.0018 & 0.9934 ± 0.0003 \\\\\n",
      "Greedy-5 c.t.p. & pure_logits & 0.9999 ± 0.0 & 0.9936 ± 0.0005 & 0.9911 ± 0.0007 & 0.9337 ± 0.0004 & 0.9371 ± 0.0009 & 0.9935 ± 0.0003 \\\\\n",
      "Greedy-5 JUCAL & pure_logits & 0.9999 ± 0.0 & 0.9943 ± 0.0003 & 0.9912 ± 0.0006 & 0.9377 ± 0.0004 & 0.9383 ± 0.001 & 0.9934 ± 0.0002 \\\\\n",
      "Greedy-50 & pure_logits & 0.9998 ± 0.0 & 0.9931 ± 0.0005 & 0.9898 ± 0.0007 & 0.9229 ± 0.0005 & 0.9369 ± 0.0014 & 0.9911 ± 0.0003 \\\\\n",
      "Greedy-50 p.t.c. & pure_logits & 0.9998 ± 0.0 & 0.9931 ± 0.0005 & 0.9898 ± 0.0007 & 0.9229 ± 0.0005 & 0.9369 ± 0.0014 & 0.9911 ± 0.0003 \\\\\n",
      "Greedy-50 c.t.p. & pure_logits & 0.9999 ± 0.0 & 0.9946 ± 0.0004 & 0.9918 ± 0.0006 & 0.9361 ± 0.0005 & 0.9411 ± 0.0013 & 0.9932 ± 0.0002 \\\\\n",
      "Greedy-50 JUCAL & pure_logits & 0.9999 ± 0.0 & 0.9948 ± 0.0004 & 0.9917 ± 0.0007 & 0.9371 ± 0.0006 & 0.9405 ± 0.0014 & 0.993 ± 0.0004 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\n",
      "EXTENDED SET SIZE TABLE:\n",
      "\\begin{tabular}{llllllll}\n",
      "\\toprule\n",
      "ensemble_type & method & \\multicolumn{6}{r}{Set Size} \\\\\n",
      " &  & DBpedia & News & SST-2 & SetFit & Tweet & IMDB \\\\\n",
      "\\midrule\n",
      "Greedy-5 & pure_logits & 1.0 ± 0.0 & 1.3517 ± 0.0385 & 1.1544 ± 0.0097 & 2.6642 ± 0.0228 & 2.3281 ± 0.0963 & 1.0996 ± 0.0065 \\\\\n",
      "Greedy-5 p.t.c. & pure_logits & 1.0 ± 0.0 & 1.3591 ± 0.0424 & 1.155 ± 0.0107 & 2.6567 ± 0.0209 & 2.3313 ± 0.0993 & 1.1003 ± 0.0062 \\\\\n",
      "Greedy-5 c.t.p. & pure_logits & 1.0 ± 0.0 & 1.3129 ± 0.0247 & 1.1511 ± 0.0129 & 2.4117 ± 0.0152 & 2.1844 ± 0.0312 & 1.1004 ± 0.0047 \\\\\n",
      "Greedy-5 JUCAL & pure_logits & 1.0 ± 0.0 & 1.249 ± 0.0161 & 1.1459 ± 0.0116 & 2.2368 ± 0.0231 & 2.1286 ± 0.0722 & 1.1004 ± 0.0039 \\\\\n",
      "Greedy-50 & pure_logits & 1.0 ± 0.0 & 1.3436 ± 0.0313 & 1.1617 ± 0.0148 & 2.6519 ± 0.0237 & 2.228 ± 0.0507 & 1.1135 ± 0.007 \\\\\n",
      "Greedy-50 p.t.c. & pure_logits & 1.0 ± 0.0 & 1.3517 ± 0.0226 & 1.1621 ± 0.0175 & 2.6514 ± 0.049 & 2.2261 ± 0.0476 & 1.114 ± 0.0092 \\\\\n",
      "Greedy-50 c.t.p. & pure_logits & 1.0 ± 0.0 & 1.2526 ± 0.0203 & 1.1394 ± 0.009 & 2.3985 ± 0.0373 & 2.1379 ± 0.035 & 1.0992 ± 0.0009 \\\\\n",
      "Greedy-50 JUCAL & pure_logits & 1.0 ± 0.0 & 1.2228 ± 0.0244 & 1.1385 ± 0.0094 & 2.2334 ± 0.0199 & 2.0633 ± 0.0291 & 1.1005 ± 0.0051 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "✓ Extended NLL table: (8, 8)\n",
      "✓ Extended AUC table: (8, 8)\n",
      "✓ Extended Set Size table: (8, 8)\n",
      "\n",
      "=== Mini Dataset Tables ===\n",
      "\n",
      "MINI AURAC TABLE:\n",
      "\\begin{tabular}{llllllll}\n",
      "\\toprule\n",
      "ensemble_type & method & \\multicolumn{6}{r}{AURAC} \\\\\n",
      " &  & DBpedia & News & SST-2 & SetFit & Tweet & IMDB \\\\\n",
      "\\midrule\n",
      "Greedy-5 & pure_logits & 0.9895 ± 0.0001 & 0.9769 ± 0.0002 & 0.979 ± 0.0008 & 0.9406 ± 0.0005 & 0.8982 ± 0.0026 & 0.9809 ± 0.0006 \\\\\n",
      "Greedy-5 p.t.c. & pure_logits & 0.9895 ± 0.0001 & 0.9769 ± 0.0003 & 0.979 ± 0.0008 & 0.9407 ± 0.0005 & 0.8981 ± 0.0026 & 0.9809 ± 0.0006 \\\\\n",
      "Greedy-5 c.t.p. & pure_logits & 0.9895 ± 0.0001 & 0.9776 ± 0.0003 & 0.9813 ± 0.0003 & 0.948 ± 0.0004 & 0.9018 ± 0.0017 & 0.9825 ± 0.0006 \\\\\n",
      "Greedy-5 JUCAL & pure_logits & 0.9895 ± 0.0001 & 0.9779 ± 0.0003 & 0.9817 ± 0.0004 & 0.95 ± 0.0005 & 0.9025 ± 0.0018 & 0.9819 ± 0.0005 \\\\\n",
      "Greedy-50 & pure_logits & 0.9893 ± 0.0 & 0.9748 ± 0.0005 & 0.9822 ± 0.0005 & 0.9503 ± 0.0003 & 0.9091 ± 0.0018 & 0.9821 ± 0.0004 \\\\\n",
      "Greedy-50 p.t.c. & pure_logits & 0.9893 ± 0.0 & 0.9748 ± 0.0005 & 0.9822 ± 0.0005 & 0.9503 ± 0.0003 & 0.9091 ± 0.0018 & 0.9821 ± 0.0004 \\\\\n",
      "Greedy-50 c.t.p. & pure_logits & 0.9895 ± 0.0 & 0.9783 ± 0.0005 & 0.9832 ± 0.0004 & 0.9556 ± 0.0003 & 0.9113 ± 0.0015 & 0.9832 ± 0.0001 \\\\\n",
      "Greedy-50 JUCAL & pure_logits & 0.9896 ± 0.0 & 0.978 ± 0.0005 & 0.9828 ± 0.0005 & 0.9554 ± 0.0002 & 0.9099 ± 0.0017 & 0.9822 ± 0.0005 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "✓ Mini AURAC table: (8, 8)\n",
      "\n",
      "MINI NLL TABLE:\n",
      "\\begin{tabular}{llllllll}\n",
      "\\toprule\n",
      "ensemble_type & method & \\multicolumn{6}{r}{NLL} \\\\\n",
      " &  & DBpedia & News & SST-2 & SetFit & Tweet & IMDB \\\\\n",
      "\\midrule\n",
      "Greedy-5 & pure_logits & 0.0432 ± 0.0009 & 0.2321 ± 0.0031 & 0.1534 ± 0.0044 & 0.4067 ± 0.002 & 0.5311 ± 0.0065 & 0.1334 ± 0.0064 \\\\\n",
      "Greedy-5 p.t.c. & pure_logits & 0.034 ± 0.0006 & 0.205 ± 0.0026 & 0.1472 ± 0.002 & 0.4051 ± 0.0018 & 0.5294 ± 0.0062 & 0.1314 ± 0.0043 \\\\\n",
      "Greedy-5 c.t.p. & pure_logits & 0.034 ± 0.0006 & 0.207 ± 0.0021 & 0.1505 ± 0.0018 & 0.4111 ± 0.001 & 0.5656 ± 0.0023 & 0.1354 ± 0.0036 \\\\\n",
      "Greedy-5 JUCAL & pure_logits & 0.0327 ± 0.0006 & 0.1966 ± 0.0026 & 0.1396 ± 0.002 & 0.3684 ± 0.0018 & 0.5205 ± 0.0059 & 0.1303 ± 0.0034 \\\\\n",
      "Greedy-50 & pure_logits & 0.0353 ± 0.0007 & 0.1967 ± 0.0032 & 0.132 ± 0.0035 & 0.3594 ± 0.0014 & 0.498 ± 0.0063 & 0.1258 ± 0.002 \\\\\n",
      "Greedy-50 p.t.c. & pure_logits & 0.0348 ± 0.0005 & 0.1964 ± 0.0031 & 0.132 ± 0.0034 & 0.3594 ± 0.0014 & 0.4979 ± 0.0061 & 0.1255 ± 0.0014 \\\\\n",
      "Greedy-50 c.t.p. & pure_logits & 0.0355 ± 0.0012 & 0.209 ± 0.0027 & 0.1486 ± 0.0014 & 0.3854 ± 0.0012 & 0.5469 ± 0.0023 & 0.1357 ± 0.0008 \\\\\n",
      "Greedy-50 JUCAL & pure_logits & 0.0306 ± 0.0006 & 0.1899 ± 0.0028 & 0.1309 ± 0.0034 & 0.348 ± 0.0013 & 0.4979 ± 0.0059 & 0.1257 ± 0.0018 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\n",
      "MINI AUC TABLE:\n",
      "\\begin{tabular}{llllllll}\n",
      "\\toprule\n",
      "ensemble_type & method & \\multicolumn{6}{r}{AUC} \\\\\n",
      " &  & DBpedia & News & SST-2 & SetFit & Tweet & IMDB \\\\\n",
      "\\midrule\n",
      "Greedy-5 & pure_logits & 0.9998 ± 0.0 & 0.9899 ± 0.0002 & 0.9853 ± 0.0007 & 0.9539 ± 0.0004 & 0.9226 ± 0.0015 & 0.9872 ± 0.0007 \\\\\n",
      "Greedy-5 p.t.c. & pure_logits & 0.9998 ± 0.0 & 0.9899 ± 0.0001 & 0.9853 ± 0.0007 & 0.9539 ± 0.0004 & 0.9225 ± 0.0015 & 0.9872 ± 0.0007 \\\\\n",
      "Greedy-5 c.t.p. & pure_logits & 0.9998 ± 0.0 & 0.9902 ± 0.0001 & 0.9868 ± 0.0004 & 0.9602 ± 0.0003 & 0.9252 ± 0.0014 & 0.9889 ± 0.0007 \\\\\n",
      "Greedy-5 JUCAL & pure_logits & 0.9998 ± 0.0 & 0.9905 ± 0.0002 & 0.9874 ± 0.0005 & 0.962 ± 0.0004 & 0.9253 ± 0.0019 & 0.9883 ± 0.0006 \\\\\n",
      "Greedy-50 & pure_logits & 0.9997 ± 0.0 & 0.9889 ± 0.0005 & 0.9878 ± 0.0008 & 0.9632 ± 0.0002 & 0.9302 ± 0.0013 & 0.9886 ± 0.0001 \\\\\n",
      "Greedy-50 p.t.c. & pure_logits & 0.9997 ± 0.0 & 0.9889 ± 0.0005 & 0.9878 ± 0.0008 & 0.9632 ± 0.0002 & 0.9302 ± 0.0013 & 0.9886 ± 0.0001 \\\\\n",
      "Greedy-50 c.t.p. & pure_logits & 0.9998 ± 0.0 & 0.9909 ± 0.0003 & 0.989 ± 0.0007 & 0.9671 ± 0.0002 & 0.9316 ± 0.0011 & 0.9897 ± 0.0001 \\\\\n",
      "Greedy-50 JUCAL & pure_logits & 0.9998 ± 0.0 & 0.9907 ± 0.0003 & 0.9885 ± 0.0008 & 0.9667 ± 0.0001 & 0.9306 ± 0.0012 & 0.9886 ± 0.0002 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\n",
      "MINI SET SIZE TABLE:\n",
      "\\begin{tabular}{llllllll}\n",
      "\\toprule\n",
      "ensemble_type & method & \\multicolumn{6}{r}{Set Size} \\\\\n",
      " &  & DBpedia & News & SST-2 & SetFit & Tweet & IMDB \\\\\n",
      "\\midrule\n",
      "Greedy-5 & pure_logits & 1.0 ± 0.0 & 1.4414 ± 0.0167 & 1.2467 ± 0.0135 & 2.2989 ± 0.0027 & 2.2997 ± 0.0356 & 1.2392 ± 0.0099 \\\\\n",
      "Greedy-5 p.t.c. & pure_logits & 1.0 ± 0.0 & 1.4475 ± 0.0184 & 1.2504 ± 0.0149 & 2.3124 ± 0.0136 & 2.3028 ± 0.0366 & 1.2347 ± 0.0158 \\\\\n",
      "Greedy-5 c.t.p. & pure_logits & 1.0 ± 0.0 & 1.4494 ± 0.0222 & 1.2251 ± 0.0104 & 2.1554 ± 0.0057 & 2.2515 ± 0.0551 & 1.1968 ± 0.0164 \\\\\n",
      "Greedy-5 JUCAL & pure_logits & 1.0 ± 0.0 & 1.4131 ± 0.0276 & 1.2091 ± 0.0115 & 1.9976 ± 0.0254 & 2.211 ± 0.0277 & 1.2148 ± 0.0115 \\\\\n",
      "Greedy-50 & pure_logits & 1.0 ± 0.0 & 1.7193 ± 0.0952 & 1.1918 ± 0.0132 & 2.1899 ± 0.0061 & 2.1735 ± 0.0475 & 1.1821 ± 0.0043 \\\\\n",
      "Greedy-50 p.t.c. & pure_logits & 1.0 ± 0.0 & 1.7274 ± 0.0792 & 1.1933 ± 0.0119 & 2.2008 ± 0.0059 & 2.1684 ± 0.0414 & 1.1831 ± 0.0092 \\\\\n",
      "Greedy-50 c.t.p. & pure_logits & 1.0 ± 0.0 & 1.4391 ± 0.0313 & 1.1862 ± 0.0127 & 2.0233 ± 0.0262 & 2.211 ± 0.0185 & 1.166 ± 0.0064 \\\\\n",
      "Greedy-50 JUCAL & pure_logits & 1.0 ± 0.0 & 1.4389 ± 0.0334 & 1.1862 ± 0.012 & 1.898 ± 0.0086 & 2.147 ± 0.0444 & 1.1819 ± 0.0126 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "✓ Mini NLL table: (8, 8)\n",
      "✓ Mini AUC table: (8, 8)\n",
      "✓ Mini Set Size table: (8, 8)\n",
      "\n",
      "Next steps:\n",
      "1. Use the new table generator functions for statistical significance formatting\n",
      "2. For FTC data, use etype_map_ftc and dataset_type='ftc'\n"
     ]
    }
   ],
   "source": [
    "# Generate LaTeX tables for all metrics with proper ensemble mappings\n",
    "print(\"Generating LaTeX tables...\")\n",
    "\n",
    "# Ensure latex directory exists\n",
    "latex_path = Path('LATEX/llm')\n",
    "latex_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Apply ensemble mappings to Extended/Mini data BEFORE creating tables\n",
    "df_extended_99_subset_mapped = df_extended_99_subset.copy()\n",
    "df_mini_99_subset_mapped = df_mini_99_subset.copy()\n",
    "\n",
    "# Apply Extended mapping\n",
    "for original, mapped in etype_map_extended.items():\n",
    "    df_extended_99_subset_mapped['ensemble_type'] = df_extended_99_subset_mapped['ensemble_type'].replace(original, mapped)\n",
    "    df_mini_99_subset_mapped['ensemble_type'] = df_mini_99_subset_mapped['ensemble_type'].replace(original, mapped)\n",
    "\n",
    "# Get ensemble types with proper ordering using the predefined patterns\n",
    "available_extended = list(df_extended_99_subset_mapped['ensemble_type'].unique())\n",
    "available_mini = list(df_mini_99_subset_mapped['ensemble_type'].unique())\n",
    "\n",
    "extended_ensemble_order = get_ensemble_order_for_dataset(available_extended, 'extended')\n",
    "mini_ensemble_order = get_ensemble_order_for_dataset(available_mini, 'mini')\n",
    "\n",
    "print(f\"Extended ensemble types (proper order): {extended_ensemble_order}\")\n",
    "print(f\"Mini ensemble types (proper order): {mini_ensemble_order}\")\n",
    "\n",
    "# Extended dataset tables\n",
    "print(\"\\n=== Extended Dataset Tables ===\")\n",
    "if 'aurac_mean±CI' in df_extended_99_subset_mapped.columns:\n",
    "    df_extended_report_aurac = create_report_view_df(\n",
    "        df_extended_99_subset_mapped, \n",
    "        value_vars=['aurac_mean±CI'], \n",
    "        calibration_method='pure_logits',\n",
    "        custom_rows=extended_ensemble_order\n",
    "    )\n",
    "    print(\"\\n EXTENDED AURAC TABLE:\")\n",
    "    print(df_extended_report_aurac.to_latex(None, index=False, escape=False))\n",
    "    print(f\"✓ Extended AURAC table: {df_extended_report_aurac.shape}\")\n",
    "\n",
    "df_extended_report_nll = create_report_view_df(\n",
    "    df_extended_99_subset_mapped, \n",
    "    value_vars=['nll_test_mean±CI'], \n",
    "    calibration_method='pure_logits',\n",
    "    custom_rows=extended_ensemble_order\n",
    ")\n",
    "df_extended_report_auc = create_report_view_df(\n",
    "    df_extended_99_subset_mapped, \n",
    "    value_vars=['auc_mean±CI'], \n",
    "    calibration_method='pure_logits',\n",
    "    custom_rows=extended_ensemble_order\n",
    ")\n",
    "df_extended_report_setsize = create_report_view_df(\n",
    "    df_extended_99_subset_mapped, \n",
    "    value_vars=['set_size_mean±CI'], \n",
    "    calibration_method='pure_logits',\n",
    "    custom_rows=extended_ensemble_order\n",
    ")\n",
    "\n",
    "print(\"\\nEXTENDED NLL TABLE:\")\n",
    "print(df_extended_report_nll.to_latex(None, index=False, escape=False))\n",
    "print(\"\\nEXTENDED AUC TABLE:\")\n",
    "print(df_extended_report_auc.to_latex(None, index=False, escape=False))\n",
    "print(\"\\nEXTENDED SET SIZE TABLE:\")\n",
    "print(df_extended_report_setsize.to_latex(None, index=False, escape=False))\n",
    "\n",
    "print(f\"✓ Extended NLL table: {df_extended_report_nll.shape}\")\n",
    "print(f\"✓ Extended AUC table: {df_extended_report_auc.shape}\")\n",
    "print(f\"✓ Extended Set Size table: {df_extended_report_setsize.shape}\")\n",
    "\n",
    "# Mini dataset tables\n",
    "print(\"\\n=== Mini Dataset Tables ===\")\n",
    "if 'aurac_mean±CI' in df_mini_99_subset_mapped.columns:\n",
    "    df_mini_report_aurac = create_report_view_df(\n",
    "        df_mini_99_subset_mapped, \n",
    "        value_vars=['aurac_mean±CI'], \n",
    "        calibration_method='pure_logits',\n",
    "        custom_rows=mini_ensemble_order\n",
    "    )\n",
    "    print(\"\\nMINI AURAC TABLE:\")\n",
    "    print(df_mini_report_aurac.to_latex(None, index=False, escape=False))\n",
    "    print(f\"✓ Mini AURAC table: {df_mini_report_aurac.shape}\")\n",
    "\n",
    "df_mini_report_nll = create_report_view_df(\n",
    "    df_mini_99_subset_mapped, \n",
    "    value_vars=['nll_test_mean±CI'], \n",
    "    calibration_method='pure_logits',\n",
    "    custom_rows=mini_ensemble_order\n",
    ")\n",
    "df_mini_report_auc = create_report_view_df(\n",
    "    df_mini_99_subset_mapped, \n",
    "    value_vars=['auc_mean±CI'], \n",
    "    calibration_method='pure_logits',\n",
    "    custom_rows=mini_ensemble_order\n",
    ")\n",
    "df_mini_report_setsize = create_report_view_df(\n",
    "    df_mini_99_subset_mapped, \n",
    "    value_vars=['set_size_mean±CI'], \n",
    "    calibration_method='pure_logits',\n",
    "    custom_rows=mini_ensemble_order\n",
    ")\n",
    "\n",
    "print(\"\\nMINI NLL TABLE:\")\n",
    "print(df_mini_report_nll.to_latex(None, index=False, escape=False))\n",
    "print(\"\\nMINI AUC TABLE:\")\n",
    "print(df_mini_report_auc.to_latex(None, index=False, escape=False))\n",
    "print(\"\\nMINI SET SIZE TABLE:\")\n",
    "print(df_mini_report_setsize.to_latex(None, index=False, escape=False))\n",
    "\n",
    "print(f\"✓ Mini NLL table: {df_mini_report_nll.shape}\")\n",
    "print(f\"✓ Mini AUC table: {df_mini_report_auc.shape}\")\n",
    "print(f\"✓ Mini Set Size table: {df_mini_report_setsize.shape}\")\n",
    "\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Use the new table generator functions for statistical significance formatting\")\n",
    "print(\"2. For FTC data, use etype_map_ftc and dataset_type='ftc'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3179lm9xdwv",
   "metadata": {},
   "source": [
    "# 📊 Generate ALL Publication-Ready LaTeX Tables with Statistical Significance\n",
    "\n",
    "**This section consolidates generation of ALL formatted tables for your ICLR paper.**\n",
    "\n",
    "- **8 tables total**: 4 Extended + 4 Mini (each with NLL, AUROC, AURAC, Set Size)\n",
    "- **All with statistical significance formatting**: Bold for best, shading for non-significant\n",
    "- **Proper LaTeX structure**: Captions, labels, booktabs formatting\n",
    "- **Consistent file naming**: `{dataset}_{metric}_formatted.tex`\n",
    "- **Correct naming**: AUC → AUROC (Area under the ROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "wrpo4gjhqj",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " GENERATING ALL PUBLICATION-READY LATEX TABLES\n",
      "============================================================\n",
      " All LaTeX tables will be saved to: LATEX/llm/iclr\n",
      " Will generate 8 publication-ready tables:\n",
      "  • Extended: ['NLL', 'AUROC', 'AURAC', 'Set_Size']\n",
      "  • Mini: ['NLL', 'AUROC', 'AURAC', 'Set_Size']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === GLOBAL CONFIGURATION ===\n",
    "print(\" GENERATING ALL PUBLICATION-READY LATEX TABLES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define paths and settings once\n",
    "latex_path = Path('LATEX/llm/iclr')\n",
    "latex_path.mkdir(parents=True, exist_ok=True)\n",
    "print(f\" All LaTeX tables will be saved to: {latex_path}\")\n",
    "\n",
    "# Define table configurations\n",
    "# Format: metric_name: (value_column, significance_df, maximize, description)\n",
    "table_configs = {\n",
    "    'extended': {\n",
    "        'NLL': ('nll_test_mean±CI', df_extended_sig_nll_99, False, \"Negative log-likelihood\"),\n",
    "        'AUROC': ('auc_mean±CI', df_extended_sig_auc_99, True, \"Area under the ROC\"),\n",
    "        'AURAC': ('aurac_mean±CI', df_extended_sig_aurac_99, True, \"Area Under the Rejection-Accuracy Curve\"),\n",
    "        'Set_Size': ('set_size_mean±CI', df_extended_sig_setsize_99, False, \"Predictive Set Size\")\n",
    "    },\n",
    "    'mini': {\n",
    "        'NLL': ('nll_test_mean±CI', df_mini_sig_nll_99, False, \"Negative log-likelihood\"),\n",
    "        'AUROC': ('auc_mean±CI', df_mini_sig_auc_99, True, \"Area under the ROC\"),\n",
    "        'AURAC': ('aurac_mean±CI', df_mini_sig_aurac_99, True, \"Area Under the Rejection-Accuracy Curve\"),\n",
    "        'Set_Size': ('set_size_mean±CI', df_mini_sig_setsize_99, False, \"Predictive Set Size\")\n",
    "    }\n",
    "}\n",
    "\n",
    "# Show summary of what will be generated\n",
    "total_tables = sum(len(configs) for configs in table_configs.values())\n",
    "print(f\" Will generate {total_tables} publication-ready tables:\")\n",
    "for dataset_name, configs in table_configs.items():\n",
    "    print(f\"  • {dataset_name.title()}: {list(configs.keys())}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "nqhwc0s4dol",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREPARING DATA FOR ALL TABLES\n",
      "----------------------------------------\n",
      "✓ Extended ensemble order: ['Greedy-5', 'Greedy-5 p.t.c.', 'Greedy-5 c.t.p.', 'Greedy-5 JUCAL', 'Greedy-50', 'Greedy-50 p.t.c.', 'Greedy-50 c.t.p.', 'Greedy-50 JUCAL']\n",
      "✓ Mini ensemble order: ['Greedy-5', 'Greedy-5 p.t.c.', 'Greedy-5 c.t.p.', 'Greedy-5 JUCAL', 'Greedy-50', 'Greedy-50 p.t.c.', 'Greedy-50 c.t.p.', 'Greedy-50 JUCAL']\n",
      "✓ Data preparation completed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === DATA PREPARATION ===\n",
    "print(\"PREPARING DATA FOR ALL TABLES\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Apply ensemble mappings to Extended/Mini data\n",
    "df_extended_mapped = df_extended_99_subset.copy()\n",
    "df_mini_mapped = df_mini_99_subset.copy()\n",
    "\n",
    "for original, mapped in etype_map_extended.items():\n",
    "    df_extended_mapped['ensemble_type'] = df_extended_mapped['ensemble_type'].replace(original, mapped)\n",
    "    df_mini_mapped['ensemble_type'] = df_mini_mapped['ensemble_type'].replace(original, mapped)\n",
    "\n",
    "# Get proper ordering using predefined patterns\n",
    "available_extended = list(df_extended_mapped['ensemble_type'].unique())\n",
    "available_mini = list(df_mini_mapped['ensemble_type'].unique())\n",
    "\n",
    "extended_order = get_ensemble_order_for_dataset(available_extended, 'extended')\n",
    "mini_order = get_ensemble_order_for_dataset(available_mini, 'mini')\n",
    "\n",
    "print(f\"✓ Extended ensemble order: {extended_order}\")\n",
    "print(f\"✓ Mini ensemble order: {mini_order}\")\n",
    "\n",
    "# Prepare data dictionary for easy access\n",
    "datasets_prepared = {\n",
    "    'extended': {\n",
    "        'data': df_extended_mapped,\n",
    "        'order': extended_order,\n",
    "        'mapping': etype_map_extended\n",
    "    },\n",
    "    'mini': {\n",
    "        'data': df_mini_mapped,\n",
    "        'order': mini_order,\n",
    "        'mapping': etype_map_extended\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"✓ Data preparation completed\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5vaocobgotm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATING ALL FORMATTED LATEX TABLES\n",
      "============================================================\n",
      "NOTE: Make sure you've restarted the kernel to load updated functions!\n",
      "\n",
      "\n",
      "Generating EXTENDED dataset tables...\n",
      "Processing NLL...\n",
      "  Saved: extended_nll_formatted.tex\n",
      "\n",
      "============================================================\n",
      "SAMPLE TABLE (properly formatted):\n",
      "============================================================\n",
      "\\begin{table}[h]\n",
      "  \\caption{Full dataset: Negative log-likelihood (\\\\( \\\\text{NLL}_{\\\\text{mean}} \\\\)) over data splits; mean ± 95\\\\% confidence interval half-width) on the full dataset (100\\\\%). The best mean is shown in bold, and methods not significantly different from the best (paired test, \\\\( \\\\alpha = 0.05 \\\\)) are shaded.}\n",
      "  \\label{tab:nll_extended_pure_logits}\n",
      "  \\centering\n",
      "  \\resizebox{\\textwidth}{!}{%\n",
      "  \\begin{tabular}{lcccccc}\n",
      "    \\toprule\n",
      "    Ensemble Type & DBpedia & News & SST-2 & SetFit & Tweet & IMDB \\\\\n",
      "    \\midrule\n",
      "    Greedy-5 & 0.0376 ± 0.0005 & 0.1682 ± 0.0048 & 0.1359 ± 0.0051 & 0.5465 ± 0.0033 & 0.5095 ± 0.0089 & 0.1171 ± 0.0028 \\\\\n",
      "    Greedy-5 p.t.c. & 0.0348 ± 0.0007 & 0.1618 ± 0.0052 & 0.1208 ± 0.004 & 0.5431 ± 0.0019 & 0.5012 ± 0.0052 & 0.1018 ± 0.0022 \\\\\n",
      "    Greedy-5 c.t.p. & 0.0324 ± 0.0006 & 0.1613 ± 0.0043 & 0.1235 ± 0.0035 & 0.54 ± 0.001 & 0.5239 ± 0.0152 & 0.104 ± 0.0021 \\\\\n",
      "    Greedy-5 JUCAL & \\cellcolor{gray!20}0.029 ± 0.0004 & 0.1479 ± 0.0023 & \\cellcolor{gray!20}0.1143 ± 0.0032 & \\cellcolor{gray!20}\\textbf{0.4965 ± 0.0013} & 0.4772 ± 0.0028 & \\cellcolor{gray!20}0.1005 ± 0.0018 \\\\\n",
      "    \\midrule\n",
      "    Greedy-50 & 0.0349 ± 0.0005 & 0.1541 ± 0.0043 & \\cellcolor{gray!20}0.1137 ± 0.0039 & 0.531 ± 0.0016 & 0.4763 ± 0.0052 & 0.105 ± 0.0026 \\\\\n",
      "    Greedy-50 p.t.c. & 0.0331 ± 0.0003 & 0.151 ± 0.0037 & \\cellcolor{gray!20}0.113 ± 0.0035 & 0.5309 ± 0.0016 & \\cellcolor{gray!20}0.4758 ± 0.0049 & 0.1042 ± 0.0019 \\\\\n",
      "    Greedy-50 c.t.p. & 0.0308 ± 0.0003 & 0.1512 ± 0.002 & 0.122 ± 0.001 & 0.5439 ± 0.0016 & 0.5112 ± 0.0027 & 0.109 ± 0.0014 \\\\\n",
      "    Greedy-50 JUCAL & \\cellcolor{gray!20}\\textbf{0.0288 ± 0.0004} & \\cellcolor{gray!20}\\textbf{0.1423 ± 0.0024} & \\cellcolor{gray!20}\\textbf{0.109 ± 0.0032} & \\cellcolor{gray!20}0.4972 ± 0.0018 & \\cellcolor{gray!20}\\textbf{0.468 ± 0.0045} & \\cellcolor{gray!20}\\textbf{0.0983 ± 0.0017} \\\\\n",
      "    \\bottomrule\n",
      "  \\end{tabular}\n",
      "  }\n",
      "\\end{table}\n",
      "============================================================\n",
      "Processing AUROC...\n",
      "  Saved: extended_auroc_formatted.tex\n",
      "Processing AURAC...\n",
      "  Saved: extended_aurac_formatted.tex\n",
      "Processing Set_Size...\n",
      "  Saved: extended_set_size_formatted.tex\n",
      "\n",
      "Generating MINI dataset tables...\n",
      "Processing NLL...\n",
      "  Saved: mini_nll_formatted.tex\n",
      "Processing AUROC...\n",
      "  Saved: mini_auroc_formatted.tex\n",
      "Processing AURAC...\n",
      "  Saved: mini_aurac_formatted.tex\n",
      "Processing Set_Size...\n",
      "  Saved: mini_set_size_formatted.tex\n",
      "\n",
      "============================================================\n",
      "GENERATION SUMMARY\n",
      "============================================================\n",
      "Successfully generated: 8 tables\n",
      "Failed to generate: 0 tables\n",
      "\n",
      "Generated files:\n",
      "  - LATEX/llm/iclr/extended_nll_formatted.tex\n",
      "  - LATEX/llm/iclr/extended_auroc_formatted.tex\n",
      "  - LATEX/llm/iclr/extended_aurac_formatted.tex\n",
      "  - LATEX/llm/iclr/extended_set_size_formatted.tex\n",
      "  - LATEX/llm/iclr/mini_nll_formatted.tex\n",
      "  - LATEX/llm/iclr/mini_auroc_formatted.tex\n",
      "  - LATEX/llm/iclr/mini_aurac_formatted.tex\n",
      "  - LATEX/llm/iclr/mini_set_size_formatted.tex\n",
      "\n",
      "All publication-ready LaTeX tables saved to: LATEX/llm/iclr\n",
      "Tables include statistical significance formatting (bold + shading)\n",
      "Ready for your ICLR paper!\n",
      "\n",
      "Note: AUC tables are now correctly named as AUROC tables\n",
      "\n",
      "============================================================\n",
      "HOW TO VIEW THE TABLES:\n",
      "============================================================\n",
      "1. Tables are saved as .tex files in LATEX/llm/\n",
      "2. To view in Jupyter, run:\n",
      "   with open('LATEX/llm/iclr/extended_nll_formatted.tex', 'r') as f:\n",
      "       print(f.read())\n",
      "3. Or copy the .tex content directly into your LaTeX document\n"
     ]
    }
   ],
   "source": [
    "# === GENERATE ALL PUBLICATION-READY TABLES ===\n",
    "# IMPORTANT: Restart kernel before running this cell to load updated functions\n",
    "print(\"GENERATING ALL FORMATTED LATEX TABLES\")  \n",
    "print(\"=\" * 60)\n",
    "print(\"NOTE: Make sure you've restarted the kernel to load updated functions!\")\n",
    "print()\n",
    "\n",
    "generated_files = []\n",
    "failed_tables = []\n",
    "\n",
    "# Process each dataset and each metric\n",
    "for dataset_name, dataset_info in datasets_prepared.items():\n",
    "    print(f\"\\nGenerating {dataset_name.upper()} dataset tables...\")\n",
    "    \n",
    "    for metric_name, (value_col, sig_df, maximize, description) in table_configs[dataset_name].items():\n",
    "        try:\n",
    "            print(f\"Processing {metric_name}...\")\n",
    "            \n",
    "            # Skip if column doesn't exist (e.g., AURAC might not be in older data)\n",
    "            if value_col not in dataset_info['data'].columns:\n",
    "                print(f\"  Skipping {metric_name}: column '{value_col}' not found\")\n",
    "                failed_tables.append(f\"{dataset_name}_{metric_name}\")\n",
    "                continue\n",
    "            \n",
    "            # Create wide-format table\n",
    "            df_wide = create_report_view_df(\n",
    "                dataset_info['data'],\n",
    "                value_vars=[value_col],\n",
    "                calibration_method='pure_logits',\n",
    "                custom_rows=dataset_info['order']\n",
    "            )\n",
    "            \n",
    "            # Generate caption with correct metric names\n",
    "            if metric_name == 'NLL':\n",
    "                metric_formula = \"\\\\\\\\( \\\\\\\\text{NLL}_{\\\\\\\\text{mean}} \\\\\\\\)\"\n",
    "            elif metric_name == 'AUROC':\n",
    "                metric_formula = \"AUROC\"\n",
    "            elif metric_name == 'AURAC':\n",
    "                metric_formula = \"AURAC\"\n",
    "            else:\n",
    "                metric_formula = metric_name.replace('_', ' ')\n",
    "            \n",
    "            # Fix dataset name for caption: Extended -> Full (FIXED)\n",
    "            caption_dataset_name = \"Full\" if dataset_name == \"extended\" else dataset_name.title()\n",
    "            \n",
    "            caption = (f\"{caption_dataset_name} dataset: {description} ({metric_formula}) over data splits; \"\n",
    "                      \"mean ± 95\\\\\\\\% confidence interval half-width) on the full dataset (100\\\\\\\\%). \"\n",
    "                      \"The best mean is shown in bold, and methods not significantly different from the best \"\n",
    "                      \"(paired test, \\\\\\\\( \\\\\\\\alpha = 0.05 \\\\\\\\)) are shaded.\")\n",
    "            \n",
    "            # Generate LaTeX table with significance formatting\n",
    "            latex_table = create_latex_table_with_significance(\n",
    "                df_wide=df_wide,\n",
    "                df_significance=sig_df,\n",
    "                ensemble_type_mapping=dataset_info['mapping'],\n",
    "                metric_name=metric_name,  # This will be 'AUROC' now instead of 'AUC'\n",
    "                dataset_name=dataset_name.title(),\n",
    "                caption=caption,\n",
    "                label=f'tab:{metric_name.lower()}_{dataset_name}_pure_logits',\n",
    "                maximize=maximize\n",
    "            )\n",
    "            \n",
    "            # Save to file with AUROC in filename\n",
    "            filename = f'{dataset_name}_{metric_name.lower()}_formatted.tex'\n",
    "            filepath = latex_path / filename\n",
    "            \n",
    "            with open(filepath, 'w') as f:\n",
    "                f.write(latex_table)\n",
    "            \n",
    "            generated_files.append(str(filepath))\n",
    "            print(f\"  Saved: {filename}\")\n",
    "            \n",
    "            # DEMO: Print first table properly formatted (FIXED)\n",
    "            if len(generated_files) == 1:\n",
    "                print(\"\\n\" + \"=\"*60)\n",
    "                print(\"SAMPLE TABLE (properly formatted):\")\n",
    "                print(\"=\"*60)\n",
    "                print(latex_table)  # This will now display with proper newlines\n",
    "                print(\"=\"*60)\n",
    "            \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"  ERROR: Failed to generate {dataset_name} {metric_name}: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "            failed_tables.append(f\"{dataset_name}_{metric_name}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"GENERATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Successfully generated: {len(generated_files)} tables\")\n",
    "print(f\"Failed to generate: {len(failed_tables)} tables\")\n",
    "\n",
    "if generated_files:\n",
    "    print(f\"\\nGenerated files:\")\n",
    "    for file in generated_files:\n",
    "        print(f\"  - {file}\")\n",
    "\n",
    "if failed_tables:\n",
    "    print(f\"\\nFailed tables:\")\n",
    "    for table in failed_tables:\n",
    "        print(f\"  - {table}\")\n",
    "\n",
    "print(f\"\\nAll publication-ready LaTeX tables saved to: {latex_path}\")\n",
    "print(\"Tables include statistical significance formatting (bold + shading)\")\n",
    "print(\"Ready for your ICLR paper!\")\n",
    "print(\"\\nNote: AUC tables are now correctly named as AUROC tables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f915d835",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
